{"cells":[{"cell_type":"markdown","metadata":{"id":"NJ8c60rUgDgN"},"source":["# **Upload section**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":524},"id":"BPHaem8AZEGm","outputId":"8dd7aa76-658a-4679-b644-9a6b0e9156a1","executionInfo":{"status":"ok","timestamp":1742886792909,"user_tz":-330,"elapsed":66172,"user":{"displayName":"Suryansh Singh","userId":"04524882864171595882"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Old resumes and data deleted. Ready for new uploads.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-3da36ae0-55bf-40f6-9733-7d40231378f9\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-3da36ae0-55bf-40f6-9733-7d40231378f9\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Naukri_Deepak[3y_0m].pdf to Naukri_Deepak[3y_0m].pdf\n","Saving Naukri_ganeshbora[1y_8m].docx to Naukri_ganeshbora[1y_8m].docx\n","Saving Naukri_RajnishSingh[0y_6m].pdf to Naukri_RajnishSingh[0y_6m].pdf\n","Saving Naukri_SagarDas[2y_0m].doc to Naukri_SagarDas[2y_0m].doc\n","Saving Naukri_ShivNarayan[1y_0m].docx to Naukri_ShivNarayan[1y_0m].docx\n","Saving Naukri_Yogeshbhati[3y_0m].docx to Naukri_Yogeshbhati[3y_0m].docx\n","Saving Parth Rustagi.pdf to Parth Rustagi.pdf\n","Saving Prashant Yadav.pdf to Prashant Yadav.pdf\n","Saving Suraj kumar.pdf to Suraj kumar.pdf\n","Uploaded and saved: /content/Naukri_Deepak[3y_0m].pdf\n","Uploaded and saved: /content/Naukri_ganeshbora[1y_8m].docx\n","Uploaded and saved: /content/Naukri_RajnishSingh[0y_6m].pdf\n","Skipped (not a resume file): Naukri_SagarDas[2y_0m].doc\n","Uploaded and saved: /content/Naukri_ShivNarayan[1y_0m].docx\n","Uploaded and saved: /content/Naukri_Yogeshbhati[3y_0m].docx\n","Uploaded and saved: /content/Parth Rustagi.pdf\n","Uploaded and saved: /content/Prashant Yadav.pdf\n","Uploaded and saved: /content/Suraj kumar.pdf\n"]}],"source":["from google.colab import files\n","import os\n","import glob\n","\n","# Define the target directory\n","upload_dir = \"/content/\"\n","\n","# List of files/folders to keep (other than resumes)\n","keep_files = {\".config\", \"sample_data\", \"Structured_Resume_Data.xlsx\", }\n","\n","# üîπ Step 1: Delete Old PDF/DOCX Resumes Silently\n","for file_path in glob.glob(os.path.join(upload_dir, \"*\")):\n","    filename = os.path.basename(file_path)\n","\n","    if filename not in keep_files and filename.endswith(('.pdf', '.docx', 'multiple_resumes_data.json')):\n","        os.remove(file_path)  # Delete the resume file or multiple_resumes_data.json\n","\n","print(\"Old resumes and data deleted. Ready for new uploads.\")  # Only one message\n","\n","# üîπ Step 2: Upload New Resumes\n","uploaded_files = files.upload()\n","\n","# Ensure uploaded_file_order maintains correct order\n","if isinstance(uploaded_files, dict):\n","    uploaded_file_order = list(uploaded_files.keys())  # If it's a dictionary\n","else:\n","    uploaded_file_order = uploaded_files  # If it's already a list\n","\n","# Move uploaded files to /content/ directory (override duplicates)\n","for filename in uploaded_file_order:\n","    if filename.endswith(('.docx', '.pdf')):  # Process only resumes\n","        file_path = os.path.join(upload_dir, filename)\n","\n","        # Save the new file (overwriting if same name exists)\n","        with open(file_path, \"wb\") as f:\n","            f.write(uploaded_files[filename])\n","\n","        print(f\"Uploaded and saved: {file_path}\")\n","    else:\n","        print(f\"Skipped (not a resume file): {filename}\")\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17561,"status":"ok","timestamp":1742886899641,"user":{"displayName":"Suryansh Singh","userId":"04524882864171595882"},"user_tz":-330},"id":"2PxVP91MZ5Pc","outputId":"b0e87f14-e159-46c1-cd2b-696572872f23"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.5)\n","Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20231228)\n","Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n","Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n","Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n","Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n","Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n","Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.1)\n","Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.12.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n","Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n","Requirement already satisfied: docx2txt in /usr/local/lib/python3.11/dist-packages (0.9)\n"]}],"source":["!pip install pdfplumber\n","!pip install python-docx\n","!pip install pandas\n","!pip install openpyxl\n","!pip install PyPDF2\n","!pip install docx2txt"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2776,"status":"ok","timestamp":1742886819522,"user":{"displayName":"Suryansh Singh","userId":"04524882864171595882"},"user_tz":-330},"id":"BBsranURSDXL","outputId":"f867b0f4-cfaf-4052-9adf-a8b9c36a33ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Content of Suraj kumar.pdf ---\n","\n","Suraj kumar\n","Account Executive\n","PROFILE SUMMARY\n","I am graduate (B.Com) having 2.5 Years of experience in Account Payable,\n","Invoice Processing, Vendor Payment Processing, Vendor Reconciliation, Bank\n","Reconciliation, Vendor Query Resolution, GST,TDS, Vendor Master Data\n","Creation and advance excel Power BI, SQL etc.. I have also good knowledge of\n","MS Excel operation like VLOOKUP, pivot table, sum if, count if, conditional\n","formatting, short cuts & other operations. Key Responsibility Area; 1) Vendor\n","Invoice processing in system(PO and Non-PO). 2) Prepare monthly Vendor\n","PERSONAL INFORMATION\n","Reconciliation. 3) Regular follow ups. 4) Handling multiple vendor queries and\n","resolve the queries as per specific TAT. 5) Update open item tracker. 6) Create\n","Ad-hoc payment. 7) Posting journal entries in the system. 8) Prepare monthly\n","Email\n","surajgupta1627@gmail.com bank reconciliation. 9) Vendor invoice verification for process in SAP. 10)\n","Vendor invoice processing in SAP with compliance of TDS and GST Laws. 11)\n","Mobile\n","Prepare and post monthly cost accrual in SAP and take care its reversal in next\n","(+91) 9911806075\n","period.\n","Total work experience\n","2 Years 6 Months\n","EDUCATION\n","KEY SKILLS\n","2017 B.Com\n","Delhi University ( SOL )\n","Accounting\n","2012 Xth\n","account payable\n","English\n","General Accounting\n","P2P\n","WORK EXPERIENCE\n","PTP\n","AP\n","Apr 2022 - Dec Account Executive\n","OTC 2024\n","E2e Research\n","R2R\n","I am graduate (B. Com) having 2.5 Years of experience in\n","Accounts Payable, Invoice Processing, Vendor Payment\n","O2C\n","Processing, Vendor Reconciliation, Bank Reconciliation,\n","AR\n","vendor Query Resolution, GST, TDS, Vendor Master Data\n","Creation & Excel Etc\n","GL\n","SAP Mar 2022 - Sep Accounts Executive\n","2024\n","E2E Research Private Limited\n","Excel\n","I am graduate (B. Com) having 2.5 Years of experience in\n","PO\n","Accounts Payable, Invoice Processing, Vendor Payment\n","GRN Processing, Vendor Reconciliation, Bank Reconciliation,\n","vendor Query Resolution, GST, TDS, Vendor Master Data\n","TDS\n","Creation & Excel Etc\n","MDM\n","Invoice processing\n","Invoice verification\n","Reconciliation\n","Billing\n","Cash Applications\n","Intercompany\n","OTHER PERSONAL DETAILS\n","City New Delhi\n","Country INDIA\n","LANGUAGES\n","English\n","\n","Total Characters Read: 2108\n","\n","--------------------------------------------------------------------------------\n","\n","\n","--- Content of Naukri_Yogeshbhati[3y_0m].docx ---\n","\n","CURRICULUM VITAE\n","\n","\n","\n","\n","Yogesh Bhati\n","       270c Mayur vihar\n","      Phase_1,pocket_2 Delhi\n","      Pin code 110091\n","\n"," Mobile No. 9643369243\n","\n"," Email: Bhati.yogesh030@gmail.com \n","\n","\n","\n","\n","\n","Objective:\n","\n","To have a dynamic career, that gives me a chance to grow economically & psychologically by exercising my knowledge and abilities in the best interest in the Ever - changing corporate scenario.\n","\n","Academic Qualification:\n","\n","\n","Strengths:\n","\n","To grow personally & professionally along the corporate leader while contributing effectively to the organization.\n","\n","     Ability to work in team.\n","\n","Positive Attitude, Analytical Skills & Fast Learner. Assertive towards with goal.\n","\n","     Punctuality & Honesty with work.\n","\n","       Skills:\n","\n","   \n","     Knowledge of SAP\n","\n","Knowledge of MS Word, Excel & Internet. Knowledge of Goods & Service tax (GST)\n","\n","      Work Experience:\n","\n","Presently Working with Oximus technologies Pvt Ltd as an Account Exe  from July-2021.\n","\n","        Job Responsibilities\n","\n","\n","Verify and validate the vendor invoices.\n","Processing the PO and Non Po invoices in ERP tool.\n","Preparation of JE tracker on daily basis.\n","Doing Vendor payment on weekly basis.\n","Preparation of Bank reco at the every Mid month.\n","Doing vendor reco at the end of every month.\n","Preparation of vendor master data in ERP Tool.\n","I Also assist in month end closure activities like GL closure and Vendor closure.\n","\n","      Hobbies:\n"," \n","      Listening Music and Travelling\n","    \n","    Personal Details:\n","\n","\n","Father‚Äôs Name\n","\n",": Mr. Vikram\n","      Date of Birth\n","30th April 1999\n","\n","\n","Nationality\n","\n","Language Known\n","\n","Marital Status\n","Salary Expect\n","\n","\n",": Indian\n","\n",": English, Hindi\n","\n",": Single\n","\n",": Negotiable\n","\n","\n","DECLERATION:\n","\n","I hereby declare that information given above is true and correct to the best of my knowledge.\n","\n","      Date:\n","\n","\n","      Place:\t(Yogesh)\n","\n","Total Characters Read: 1755\n","\n","--------------------------------------------------------------------------------\n","\n","\n","--- Content of Naukri_RajnishSingh[0y_6m].pdf ---\n","\n","CURRICULUMVITAE\n","Rajnish Singh\n","Contact:\n",": Kumarrj479@gmail.com\n","ÔÇ∑ :+918527287985\n",":C-30 Inderpuri j.j colony Budh Nagar New Delhi 110012\n","Objective:\n","I would like to be a part of an organization where I can contribute positively my skill for growth of an\n","organization and seeking a position to utilize my skills and abilities in a company that offers Professional as\n","well as personal growth while being resourceful, innovative, and flexible.\n","Education Qualification:\n","SCHOOL/ COLLEGE YEAR OF AWARD\n","EXAMINATION BOARD PERCENTAGE\n","PASSING OF CLASS\n","Rajkiya Pratibha\n","Vikas Vidyalaya.\n","cbse board 2012 74.1 1st Class\n","th Hari nagar, new\n","10\n","delhi\n","Rajkiya pratibha\n","vikas vidyalaya.hari cbse board 2014 85.74 First Class\n","th\n","12\n","nagar, new delhi\n","School of open\n","Delhi\n","B.com (hons.) learning,delhi 2018 53.88 2nd Class\n","university\n","university\n","Assets:\n","ÔÉò Adjustable to any environment.\n","ÔÉò Highly motivated to work as a team.\n","ÔÉò Believe on Smart Work\n","ÔÉò Effective Communication skill\n","Skills:\n","ÔÉò Formal Mail Writing\n","ÔÉò M.S Excel\n","ÔÉò M.S Word\n","ÔÉò Tally\n","ÔÉò M.S Point\n","ÔÉò Bank Reconciliation\n","ÔÉò Decision Making\n","ÔÉò Team Management\n","ÔÉò Certified in computer concepts, demonstrating foundational computer and internet skills.\n","ÔÉò Working experience with accounting software such as QuickBooks and Descartes.\n","Work Experience:\n","ÔÅ∂ Sky2C Freight System Inc.ÔÄ†\n","ÔÉò Accounts Payable Department\n","ÔÇ∑ Vendor ledger Reconcile\n","ÔÇ∑ Bank Reconciliation\n","ÔÇ∑ Prepare AP and AR report\n","ÔÇ∑ Sends Mails on daily basis\n","ÔÅ∂ Sky Freight System LLc.ÔÄ†\n","ÔÅ¨ Vendor ledger Reconcile\n","ÔÅ¨ Bank Reconciliation\n","ÔÅ¨ Prepare AP & AR report\n","ÔÅ¨ Sends Mails on daily basis\n","ÔÅ∂ Glaze trading India Pvt Ltd.ÔÄ†\n","ÔÇ∑ Direct Marketing\n","Personal Data:\n","ÔÉò Father‚Äôs Name:\n","Jai Prakash Singh\n","ÔÉò Date of Birth:\n","25/06/1997\n","ÔÉò Sex:\n","Male\n","ÔÉò Nationality:\n","Indian\n","ÔÉò Marital Status:\n","Unmarried\n","ÔÉò Languages Known:\n","Hindi, English\n","Declaration\n","I here by declare that all statement made in this application are true complete and correct to the best of\n","my knowledge and belief.\n","Date:--/--/----\n","Place: Delhi (Rajnish Singh)\n","\n","Total Characters Read: 1978\n","\n","--------------------------------------------------------------------------------\n","\n","\n","--- Content of Naukri_Deepak[3y_0m].pdf ---\n","\n","Resume\n","Deepak\n","Address: H.No-103 Gali.No-7 Tirkha Colony Ballabgarh Fbd-121004\n","Mobile No: 8059905779\n","E Mail ID: deepakgoswami29395@gmail.com\n","Resume Summary\n","Finance Professional who aspires to work in challenging environment where my qualification\n","and my skills can add value to the organization\n","Academic Qualification\n","ÔÇ∑ Passed B. Com from MAHGU in 2020 with 65.00%\n","ÔÇ∑ Passed 12th from Haryana Board in 2013 with 60.00%\n","ÔÇ∑ Passed 10th from Haryana Board in 2010 with 63.00%\n","Organizational Experience\n","ÔÇ∑ Working in Imigious Technologies PVT LTD as Account Executive from Jan 2024 to Till Date.\n","ÔÇ∑ 2 Year Experience in Indication Instrument LTD as Account Executive.\n","Key Responsibility Area & Skills:\n","ÔÇ∑ Vendor Invoice Verification for Process in SAP\n","ÔÇ∑ Vendor Invoice Processing in SAP with compliance of TDS & GST Laws\n","ÔÇ∑ Process 100 invoices in SAP on daily basis\n","ÔÇ∑ Process PO & Non-PO based invoices in system on daily basis\n","ÔÇ∑ Prepare & Post Monthly Cost accrual in SAP and take care its reversal in next period\n","ÔÇ∑ Prepare Account Payable Reconciliation on monthly basis\n","ÔÇ∑ Handling all reclass & reversal of entries as and when required for corrections\n","ÔÇ∑ Handling multiple vendor queries and resolve these queries as per specified TAT\n","ÔÇ∑ Vendor Master Data creation in SAP with required approvals\n","ÔÇ∑ Vendor Payment Processing in SAP (Exposure through cross training)\n","IT Skills:\n","ÔÇ∑ MS Office (MS Word, MS Excel & MS Power Point)\n","ÔÇ∑ SAP Operation & T. Codes knowledge related to Accounts Payable Profile\n","Personal Attributes:\n","ÔÇ∑ Positive & Learning attitude.\n","ÔÇ∑ Interested in learning new things.\n","Personal Profile:\n","Father‚Äôs Name: Mr.Amar Giri\n","Date of Birth: 29.Mar.1995\n","Marital status: Married\n","Nationality: Indian\n","Gender: Male\n","Languages: Hindi & English\n","I do hereby declare that the above information given by me is true to the best of my\n","knowledge and belief.\n","Place: FARIDABAD\n","Date: Deepak\n","\n","Total Characters Read: 1873\n","\n","--------------------------------------------------------------------------------\n","\n","\n","--- Content of Parth Rustagi.pdf ---\n","\n","PARTH RUSTAGI\n","Contact No. :- +91-8950624410\n","E-Mail : parthrustagi2410@gmail.com\n","CAREER OBJECTIVE\n","To work in a challenging and dynamic environment and to keep adding value to the organization that I\n","represent and serve, while also concurrently upgrading my skills and knowledge.\n","EMPLOYMENT RECITAL\n","ÔÄ†ÔÄ†ÔÄ†ÔÉúÔÄ†Presently Working in PRECISE LOGISTIC SOLUTION PRIVATE LIMITED from Feb 2024 and\n","work profile includes :-\n","ÔÇ∑ GST Returns( GSTR-1 ,GSTR-3B)\n","ÔÇ∑ ITC Reconciliation from GSTR-2B vis a vis books.\n","ÔÇ∑ Responsible for preparation of TDS (Payment & Returns).\n","ÔÇ∑ Journal Posting.\n","ÔÇ∑ Responsible for Bank Reconciliation Statement on monthly basis.\n","ÔÇ∑ Vendor Reconciliation as per requirement.\n","ÔÇ∑ Invoice related compliances such as E-way Bills and E-Invoicing in compliance with GST Rules.\n","ÔÉúÔÄ†M/s INVESTOMART WEALTH MANAGEMENT PRIVATE LIMITED from Feb 2023 to Feb 2024\n","ÔÄ†ÔÄ†and work profile includes\n","ÔÇ∑ Responsible for TDS data for Returns and payments.\n","ÔÇ∑ GST Returns preparation.\n","ÔÇ∑ Book-keeping and JV Posting.\n","EDUCATION\n","ÔÉú Professional Qualification:\n","ÔÇ∑ Pursuing Bachelor of Commerce from Delhi University.\n","ÔÇ∑ Completed Online Certifications from CFI (Corporate Finance Institute) in\n","1. Reading Financial Statements\n","ÔÄ†\n","ÔÉú Educational Qualification:\n","XIIth CBSE Board(2021) 84%\n","Xth CBSE Board(2019) 81%\n","CORE COMPETENCIES\n","‚óè Self motivated, Adaptable\n","‚óè Quick learner\n","‚óè Problem-solving and organizational skills\n","‚óè Ability to drive results in a fast-paced/deadline-oriented environment\n","IT SKILLS\n","Well versed with :\n","o MS EXCEL ( VLOOKUP,HLOOKUP,PIVOT TABLE ).\n","o TALLY PRIME\n","o BUSY\n","PERSONAL DOSSIER\n","Father Name : Mr. Gopal Dass\n","Date of Birth : 24-10-2003\n","Gender : Male\n","Language : Hindi, English\n","Address : Rewari, Haryana\n","Date :\n","Place : Rewari\n","\n","Total Characters Read: 1710\n","\n","--------------------------------------------------------------------------------\n","\n","\n","--- Content of Naukri_ShivNarayan[1y_0m].docx ---\n","\n","\n","\n","Total Characters Read: 0\n","\n","--------------------------------------------------------------------------------\n","\n","\n","--- Content of Prashant Yadav.pdf ---\n","\n","PRASHANT YADAV\n","Gurugram\n","prashantydv1406@gmail.com | 8295971406\n","18/09/2001\n","OBJECTIVE\n","\"Seeking a leadership role where I can apply my skills to improve efficiency and profitability.\"\n","ACADEMIC QUALIFICATIONS\n","INDIRA GANDHI NATIONAL OPEN UNIVERSITY, NEW DELHI\n","Pursuing MBA IN FINANCE\n","INDIRA GANDHI UNIVERSITY, MEERPUR REWARI\n","2019-2022\n","BCOM\n","YADUVANSHI SHIKSHA NIKETAN REWARI\n","2019\n","Senior Secondary\n","YADUVANSHI SHIKSHA NIKETAN REWARI\n","2017\n","Matriculation\n","OTHER QUALIFICATIONS\n","Completed certificate course in financial accounting\n","WORK EXPERIENCE\n","MKNA and Associates\n","1 October 2023\n","Accounts Executive\n","Work Responsibility Include -\n","* Managing Accounts\n","* Preparing Various Reports On MS-EXCEL\n","* Filling TDS/TCS Returns\n","* Filling GST Returns\n","TECHNICAL SKILLS\n","Accounting Software\n","Advance Excel\n","Project Report\n","Cost Sheet\n","TDS/TCS Returns\n","GST Registration and Returns\n","Bank Reconciliation Statement\n","OTHER COMPUTER SKILLS\n","ITR filling\n","Advanced Excel (Vlookup , Pivot table etc)\n","E-way Bill and E-Invoice preparation\n","GST returns filling\n","Tally Prime\n","Busy Accounting Software\n","CERTIFICATE\n","SEBI Investor Certification Examination\n","STRENGTHS\n","Team Work\n","Honesty\n","ACHIEVEMENTS & AWARDS\n","Secured International Rank 60 and Zonal Rank 12 in Haryana zone in Science\n","Olympiad Foundation (SOF)\n","International Company Secretary Exam\n","HOBBIES\n","Swimming\n","Learning\n","LANGUAGE\n","English\n","Hindi\n","ADDITIONAL INFORMATION\n","Father Name - Ajit Singh\n","Gender - Male\n","Martial Status - Unmarried\n","Communication Address - V.PO. Gokalgarh, Rewari, Haryana\n","\n","Total Characters Read: 1483\n","\n","--------------------------------------------------------------------------------\n","\n","\n","--- Content of Naukri_ganeshbora[1y_8m].docx ---\n","\n","CURRICULUM VITAE\n","Ganesh Bora\n","Mobile Number : +91 8376087424\n","E-mail : ganeshbora25@gmail.com\n","\n","\n","OBJECTIVE:-\n","Looking for an organization where I could enhance my knowledge and experience on legal aspects while working with experienced person.\n","\n","\n","Professional Experience:- \n","\n","Audit Assistant\n","[Raas and Associates]\n","[Jangpura,Delhji]\n","[27-Feb-2023 To 19-June-2023].\n","Verifying financial statements, including balance sheets, income statements, and cash flow statements\n","General Accountant\n","[Ram Kumar Parveen Kumar Jeweller‚Äôs]\n","[Jangpura,Delhi]\n","[20-June-2023 ‚Äì currently working]\n","Handled accounts payable and accounts receivables functions, including processing invoices and issuing payments.\n","Prepared bank reconciliations and maintained accurate financial records.\n","Assisted in the preparation of financial statements and management reports.\n","Conducted month-end closing procedures and reconciled general ledger accounts.\n","Education:\n","Bachelor in Commerce \n","[Delhi University]\n","[Completed in 2023]\n","Diploma in E-Accounting: - Finance, Taxation, Auditing.\n","Skills:\n","Proficient in financial reporting software (e.g., Tally Prime).\n","Strong analytical skills with the ability to interpret financial data and provide insights.\n","Excellent attention to detail and organizational skills.\n","understanding of financial regulations.\n","Effective communication and interpersonal skills.\n","\n","Total Characters Read: 1346\n","\n","--------------------------------------------------------------------------------\n","\n"]}],"source":["import os\n","import pdfplumber\n","import docx\n","\n","# Directory for QAuploaded files\n","upload_dir = \"/content/\"\n","\n","# Function to extract text from PDF\n","def read_pdf(pdf_path):\n","    text = \"\"\n","    try:\n","        with pdfplumber.open(pdf_path) as pdf:\n","            for page in pdf.pages:\n","                extracted_text = page.extract_text()\n","                if extracted_text:  # Ensure text is not None\n","                    text += extracted_text + \"\\n\"\n","    except Exception as e:\n","        print(f\"Error reading {pdf_path}: {e}\")\n","    return text.strip()\n","\n","# Function to extract text from DOCX\n","def read_docx(docx_path):\n","    text = \"\"\n","    try:\n","        doc = docx.Document(docx_path)\n","        for para in doc.paragraphs:\n","            text += para.text + \"\\n\"\n","    except Exception as e:\n","        print(f\"Error reading {docx_path}: {e}\")\n","    return text.strip()\n","\n","# Process all uploaded files and display content\n","for filename in os.listdir(upload_dir):\n","    file_path = os.path.join(upload_dir, filename)\n","    text_content = \"\"\n","\n","    if filename.endswith(\".pdf\"):\n","        text_content = read_pdf(file_path)\n","    elif filename.endswith(\".docx\"):\n","        text_content = read_docx(file_path)\n","    else:\n","        continue  # Skip unsupported files\n","\n","    print(f\"\\n--- Content of {filename} ---\\n\")\n","    print(text_content)  # Display full document text\n","    print(f\"\\nTotal Characters Read: {len(text_content)}\")  # Confirm full file reading\n","    print(\"\\n\" + \"-\" * 80 + \"\\n\")  # Separator for readability"]},{"cell_type":"markdown","metadata":{"id":"1CQMXwUjzb93"},"source":["# **Name**"]},{"cell_type":"markdown","metadata":{"id":"W6PcAjdx7VJx"},"source":["**Cell 1**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1519,"status":"ok","timestamp":1742798512333,"user":{"displayName":"Suryansh Singh","userId":"04524882864171595882"},"user_tz":-330},"id":"bgT1qDGLzbhP","outputId":"7cd46650-5169-44c0-903f-8cb3edab1de5"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Extracted Data from Abhishek Raj.pdf ---\n","Candidate Name : Abhishek Raj\n","--------------------------------------------------------------------------------\n","\n","--- Extracted Data from Naukri_PradeepKumar[3y_0m].pdf ---\n","Candidate Name : PradeepKumar\n","--------------------------------------------------------------------------------\n","\n","--- Extracted Data from Naukri_PrashantSinghal[8y_0m].pdf ---\n","Candidate Name : PrashantSinghal\n","--------------------------------------------------------------------------------\n","\n","--- Extracted Data from Parth Rustagi.pdf ---\n","Candidate Name : Parth Rustagi\n","--------------------------------------------------------------------------------\n"]}],"source":["import os\n","import pdfplumber\n","import docx\n","import re\n","\n","# Directory for uploaded files\n","upload_dir = \"/content/\"\n","\n","# Unwanted terms to remove from names\n","UNWANTED_TERMS = {\"ympdf\", \"pdf\", \"Naukri\", \"ymdocx\", \"ym\", \"docx\", \"resume\", \"Resume\"}\n","\n","# Function to clean names by removing unwanted terms\n","def clean_name(name):\n","    if name:\n","        for term in UNWANTED_TERMS:\n","            name = name.replace(term, \"\")\n","        return name.strip()\n","    return None\n","\n","# Function to extract text from PDF\n","def read_pdf(pdf_path):\n","    text = \"\"\n","    try:\n","        with pdfplumber.open(pdf_path) as pdf:\n","            for page in pdf.pages:\n","                extracted_text = page.extract_text()\n","                if extracted_text:\n","                    text += extracted_text + \"\\n\"\n","    except Exception as e:\n","        print(f\"Error reading {pdf_path}: {e}\")\n","    return text.strip()\n","\n","# Function to extract text from DOCX\n","def read_docx(docx_path):\n","    text = \"\"\n","    try:\n","        doc = docx.Document(docx_path)\n","        for para in doc.paragraphs:\n","            text += para.text + \"\\n\"\n","    except Exception as e:\n","        print(f\"Error reading {docx_path}: {e}\")\n","    return text.strip()\n","\n","# Function to extract candidate's name from the filename\n","def extract_name_from_filename(filename):\n","    filename_clean = re.sub(r'[^a-zA-Z\\s]', '', filename).strip()  # Remove numbers and special chars\n","    possible_name = \" \".join(filename_clean.split()[:3])  # Take first three words as name\n","    return clean_name(possible_name)\n","\n","# Process files in the uploaded order\n","uploaded_file_order = list(uploaded_files.keys())  # Keeps order of uploaded files\n","name_results = {}\n","\n","for filename in uploaded_file_order:\n","    file_path = os.path.join(upload_dir, filename)\n","    text_content = \"\"\n","\n","    if filename.endswith(\".pdf\"):\n","        text_content = read_pdf(file_path)\n","    elif filename.endswith(\".docx\"):\n","        text_content = read_docx(file_path)\n","    else:\n","        continue  # Skip unsupported files\n","\n","    # If content was successfully extracted, process name\n","    if text_content:\n","        # Extract Name from filename instead of document text\n","        name = extract_name_from_filename(filename)\n","        name_results[filename] = {\"Name\": name}\n","    else:\n","        print(f\"Skipped {filename}: Unable to extract text.\")\n","\n","# Print results in the order of upload\n","for filename in uploaded_file_order:\n","    if filename in name_results:  # Only print if name was extracted\n","        print(f\"\\n--- Extracted Data from {filename} ---\")\n","        print(f\"Candidate Name : {name_results[filename]['Name']}\")\n","        print(\"-\" * 80)\n","    else:\n","        print(f\"--- Skipped {filename} --- No data extracted.\")\n"]},{"cell_type":"markdown","metadata":{"id":"3vkBHEuP7eHz"},"source":["**Cell 2**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1742798512359,"user":{"displayName":"Suryansh Singh","userId":"04524882864171595882"},"user_tz":-330},"id":"ouBfXp4Ys8Ok","outputId":"09e1b8b5-fcdc-4f1d-f2ca-616e18256633"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Extracted Data from Abhishek Raj.pdf ---\n","Candidate Name : Abhishek  Raj\n","--------------------------------------------------------------------------------\n","\n","--- Extracted Data from Naukri_PradeepKumar[3y_0m].pdf ---\n","Candidate Name : Pradeep Kumar\n","--------------------------------------------------------------------------------\n","\n","--- Extracted Data from Naukri_PrashantSinghal[8y_0m].pdf ---\n","Candidate Name : Prashant Singhal\n","--------------------------------------------------------------------------------\n","\n","--- Extracted Data from Parth Rustagi.pdf ---\n","Candidate Name : Parth  Rustagi\n","--------------------------------------------------------------------------------\n"]}],"source":["import re\n","\n","# Function to add spaces between capital letters in names\n","def format_name(name):\n","    return re.sub(r'(?<!^)(?=[A-Z])', ' ', name).strip() if name else \"Name Not Found\"\n","\n","# Apply the formatting function to all extracted names\n","for filename in name_results:\n","    name_results[filename][\"Name\"] = format_name(name_results[filename][\"Name\"])\n","\n","# Display the formatted extracted names\n","if not name_results:\n","    print(\"No extracted names found. Please ensure the name extraction code has been executed.\")\n","else:\n","    for filename, data in name_results.items():\n","        extracted_name = data.get(\"Name\", \"Name Not Found\")  # Fallback in case of missing names\n","        print(f\"\\n--- Extracted Data from {filename} ---\")\n","        print(f\"Candidate Name : {extracted_name}\")\n","        print(\"-\" * 80)\n"]},{"cell_type":"markdown","metadata":{"id":"b5oYtftl7jlD"},"source":["**Cell 3**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51,"status":"ok","timestamp":1742798512412,"user":{"displayName":"Suryansh Singh","userId":"04524882864171595882"},"user_tz":-330},"id":"fXAK2WzEu6zE","outputId":"9a3fa70d-7791-4bd8-eebb-be4e96817044"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Extracted Data from Abhishek Raj.pdf ---\n","First Name  : Abhishek\n","Last Name   : Raj\n","--------------------------------------------------------------------------------\n","\n","--- Extracted Data from Naukri_PradeepKumar[3y_0m].pdf ---\n","First Name  : Pradeep\n","Last Name   : Kumar\n","--------------------------------------------------------------------------------\n","\n","--- Extracted Data from Naukri_PrashantSinghal[8y_0m].pdf ---\n","First Name  : Prashant\n","Last Name   : Singhal\n","--------------------------------------------------------------------------------\n","\n","--- Extracted Data from Parth Rustagi.pdf ---\n","First Name  : Parth\n","Last Name   : Rustagi\n","--------------------------------------------------------------------------------\n"]}],"source":["# Function to split names into First, Middle, and Last Name\n","def split_name(full_name):\n","    name_parts = full_name.split()\n","    first_name = name_parts[0] if len(name_parts) > 0 else \"Not Found\"\n","    middle_name = \" \".join(name_parts[1:-1]) if len(name_parts) > 2 else \"\"  # Leave blank if no middle name\n","    last_name = name_parts[-1] if len(name_parts) > 1 else \"\"  # Leave blank if no last name\n","\n","    return first_name, middle_name, last_name\n","\n","# Apply the splitting function to all extracted names\n","split_name_results = {}\n","\n","for filename, data in name_results.items():\n","    formatted_name = data.get(\"Name\", \"Name Not Found\")\n","    first, middle, last = split_name(formatted_name)\n","\n","    split_name_results[filename] = {\n","        \"First Name\": first,\n","        \"Middle Name\": middle,\n","        \"Last Name\": last\n","    }\n","\n","# Display the split names in a structured format\n","if not split_name_results:\n","    print(\"No formatted names found. Please ensure the name formatting code has been executed.\")\n","else:\n","    for filename, data in split_name_results.items():\n","        print(f\"\\n--- Extracted Data from {filename} ---\")\n","        print(f\"First Name  : {data['First Name']}\")\n","        if data[\"Middle Name\"]:  # Only display Middle Name if it's not empty\n","            print(f\"Middle Name : {data['Middle Name']}\")\n","        print(f\"Last Name   : {data['Last Name']}\")\n","        print(\"-\" * 80)\n"]},{"cell_type":"markdown","metadata":{"id":"09KFGH_dcyix"},"source":["# **Father name**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2443,"status":"ok","timestamp":1742798514856,"user":{"displayName":"Suryansh Singh","userId":"04524882864171595882"},"user_tz":-330},"id":"pLsjIMtCcyBR","outputId":"ff677eed-05e8-4379-cdc2-dbbb82f1aad2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Resume: Abhishek Raj.pdf\n","Father's Name: \n","\n","Resume: Naukri_PrashantSinghal[8y_0m].pdf\n","Father's Name: \n","\n","Resume: Naukri_PradeepKumar[3y_0m].pdf\n","Father's Name: \n","\n","Resume: Parth Rustagi.pdf\n","Father's Name: Mr. Gopal Dass\n","\n"]}],"source":["import os\n","import re\n","import pdfplumber\n","import docx\n","\n","def extract_text_from_docx(docx_path):\n","    \"\"\"Extracts text from a DOCX file.\"\"\"\n","    doc = docx.Document(docx_path)\n","    return '\\n'.join([para.text for para in doc.paragraphs])\n","\n","def extract_text_from_pdf(pdf_path):\n","    \"\"\"Extracts text from a PDF file.\"\"\"\n","    text = \"\"\n","    with pdfplumber.open(pdf_path) as pdf:\n","        for page in pdf.pages:\n","            extracted_text = page.extract_text()\n","            if extracted_text:\n","                text += extracted_text + \"\\n\"\n","    return text.strip()\n","\n","def extract_father_name(text):\n","    \"\"\"Extracts Father's Name from the resume text.\"\"\"\n","    father_name_pattern = re.compile(r\"Father[‚Äô'`]?s? Name[\\s\\-:\\t]+(Mr\\.\\s[A-Z][a-z]+(?:\\s[A-Z][a-z]+)*)\", re.IGNORECASE)\n","    match = father_name_pattern.search(text)\n","\n","    if match:\n","        father_name = match.group(1).strip()\n","        # Ensure \"Date of Birth\" or extra text is not mistakenly included\n","        father_name = re.split(r\"\\s*Date of Birth.*\", father_name)[0].strip()\n","        return father_name\n","\n","    return \"\"  # Return blank if Father's Name not found\n","\n","# Dictionary to store extracted Father Names\n","father_name_results = {}\n","\n","# Process each uploaded resume\n","upload_dir = \"/content/\"\n","resume_files = [f for f in os.listdir(upload_dir) if f.endswith(('.pdf', '.docx'))]\n","\n","for resume in resume_files:\n","    file_path = os.path.join(upload_dir, resume)\n","\n","    # Extract text based on file type\n","    if resume.endswith(\".docx\"):\n","        text = extract_text_from_docx(file_path)\n","    elif resume.endswith(\".pdf\"):\n","        text = extract_text_from_pdf(file_path)\n","    else:\n","        continue\n","\n","    # Extract Father's Name\n","    father_name = extract_father_name(text)\n","\n","    # Store result\n","    father_name_results[resume] = father_name\n","\n","    print(f\"Resume: {resume}\\nFather's Name: {father_name}\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"eG6Qyg8SULH0"},"source":["# **Gender, language and Email id**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5933,"status":"ok","timestamp":1742798520817,"user":{"displayName":"Suryansh Singh","userId":"04524882864171595882"},"user_tz":-330},"id":"VzjFSR0OUK0s","outputId":"f794fd7f-be48-4bf0-826f-a9d794b73c22"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Extracted Data from Abhishek Raj.pdf ---\n","Gender: \n","Languages: \n","Email ID: abhishekraj270699@gmail.com\n","--------------------------------------------------------------------------------\n","\n","--- Extracted Data from Naukri_PradeepKumar[3y_0m].pdf ---\n","Gender: \n","Languages: \n","Email ID: pradeep.info1720@gmail.com\n","--------------------------------------------------------------------------------\n","\n","--- Extracted Data from Naukri_PrashantSinghal[8y_0m].pdf ---\n","Gender: \n","Languages: Hindi, English\n","Email ID: prashant.singhal9@gmail.com\n","--------------------------------------------------------------------------------\n","\n","--- Extracted Data from Parth Rustagi.pdf ---\n","Gender: Male\n","Languages: Hindi, English\n","Email ID: parthrustagi2410@gmail.com\n","--------------------------------------------------------------------------------\n"]}],"source":["import os\n","import pdfplumber\n","import docx\n","import re\n","\n","# Directory for uploaded files\n","upload_dir = \"/content/\"\n","\n","# List of languages to detect\n","LANGUAGES = {\"english\", \"hindi\", \"bengali\", \"tamil\", \"telugu\", \"marathi\", \"gujarati\", \"punjabi\"}\n","\n","# Function to extract text from PDF\n","def read_pdf(pdf_path):\n","    text = \"\"\n","    try:\n","        with pdfplumber.open(pdf_path) as pdf:\n","            for page in pdf.pages:\n","                extracted_text = page.extract_text()\n","                if extracted_text:\n","                    text += extracted_text + \"\\n\"\n","    except Exception as e:\n","        print(f\"Error reading {pdf_path}: {e}\")\n","    return text.strip()\n","\n","# Function to extract text from DOCX\n","def read_docx(docx_path):\n","    text = \"\"\n","    try:\n","        doc = docx.Document(docx_path)\n","        for para in doc.paragraphs:\n","            text += para.text + \"\\n\"\n","    except Exception as e:\n","        print(f\"Error reading {docx_path}: {e}\")\n","    return text.strip()\n","\n","# Function to label Gender, Language, and Email ID\n","def label_gender_language_email(text):\n","    gender = None\n","    language = []\n","    email_id = None\n","\n","    # Rule 1: Detect Gender (Male/Female)\n","    gender_match = re.findall(r'\\b(Male|Female)\\b', text, re.IGNORECASE)\n","    if gender_match:\n","        gender = gender_match[0].capitalize()\n","\n","    # Rule 2: Detect Languages from predefined list\n","    for lang in LANGUAGES:\n","        if re.search(r'\\b' + re.escape(lang) + r'\\b', text, re.IGNORECASE):\n","            language.append(lang.capitalize())\n","\n","    # Rule 3: Detect Email ID using regex pattern (now includes possible \"EMAIL ID.\" prefix)\n","    email_match = re.findall(r'(?:EMAIL\\sID\\.\\s)?[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', text)\n","    if email_match:\n","        email_id = email_match[0]  # Take the first detected email\n","\n","    return gender, language, email_id\n","\n","# Process files in uploaded order\n","uploaded_file_order = list(uploaded_files.keys())  # Maintain upload order\n","results = {}\n","\n","for filename in uploaded_file_order:\n","    file_path = os.path.join(upload_dir, filename)\n","    text_content = \"\"\n","\n","    if filename.endswith(\".pdf\"):\n","        text_content = read_pdf(file_path)\n","    elif filename.endswith(\".docx\"):\n","        text_content = read_docx(file_path)\n","    else:\n","        continue  # Skip unsupported files\n","\n","    gender, language, email_id = label_gender_language_email(text_content)\n","\n","    results[filename] = {\n","        \"Gender\": gender,\n","        \"Languages\": language,\n","        \"Email ID\": email_id\n","    }\n","\n","# Print extracted data\n","for file, data in results.items():\n","    print(f\"\\n--- Extracted Data from {file} ---\")\n","    print(f\"Gender: {data['Gender'] if data['Gender'] else ''}\")  # Leave blank if None\n","    print(f\"Languages: {', '.join(data['Languages']) if data['Languages'] else ''}\")  # Leave blank if None\n","    print(f\"Email ID: {data['Email ID'] if data['Email ID'] else ''}\")  # Leave blank if None\n","    print(\"-\" * 80)"]},{"cell_type":"markdown","metadata":{"id":"UkbHXLNkMYlV"},"source":["# **DOB and Phone no.**"]},{"cell_type":"code","source":["import os\n","import pdfplumber\n","import docx\n","import re\n","\n","# Directory for uploaded files\n","upload_dir = \"/content/\"\n","\n","# Function to extract text from PDF\n","def read_pdf(pdf_path):\n","    text = \"\"\n","    try:\n","        with pdfplumber.open(pdf_path) as pdf:\n","            for page in pdf.pages:\n","                extracted_text = page.extract_text()\n","                if extracted_text:\n","                    text += extracted_text + \"\\n\"\n","    except Exception as e:\n","        print(f\"Error reading {pdf_path}: {e}\")\n","    return text.strip()\n","\n","# Function to extract text from DOCX\n","def read_docx(docx_path):\n","    text = \"\"\n","    try:\n","        doc = docx.Document(docx_path)\n","        for para in doc.paragraphs:\n","            text += para.text + \"\\n\"\n","    except Exception as e:\n","        print(f\"Error reading {docx_path}: {e}\")\n","    return text.strip()\n","\n","# Function to label Date of Birth (DOB) and Phone Number\n","def label_dob_phone(text):\n","    dob = None\n","    phone_numbers = []\n","\n","    # Rule 1: Extract DOB if explicitly labeled\n","    dob_match = re.findall(r'Date\\s*of\\s*Birth\\s*:\\s*(\\d{1,2}-\\d{1,2}-\\d{4})', text, re.IGNORECASE)\n","    if dob_match:\n","        dob = dob_match[0].strip()\n","\n","    # Rule 2: Identify potential DOB (Pattern: DD-MM-YYYY)\n","    generic_dob_match = re.findall(r'\\b\\d{1,2}-\\d{1,2}-\\d{4}\\b', text)\n","    if generic_dob_match and not dob:\n","        dob = generic_dob_match[0]  # Use first detected date\n","\n","    # Rule 3: Extract DOB in format like \"8th Mar 2001\"\n","    dob_format_match = re.findall(r'\\b(\\d{1,2}(?:st|nd|rd|th)?\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{4})\\b', text, re.IGNORECASE)\n","    if dob_format_match and not dob:\n","        dob = dob_format_match[0]  # Use first detected date\n","\n","    # Rule 4: Extract DOB in format like \"22/01/2000\"\n","    dob_slash_format_match = re.findall(r'\\b(\\d{1,2}/\\d{1,2}/\\d{4})\\b', text)\n","    if dob_slash_format_match and not dob:\n","        dob = dob_slash_format_match[0]  # Use first detected date\n","\n","    # Rule 5: Extract Phone Numbers with prioritized checking\n","    phone_patterns = [\n","        r'\\+91[-\\s]?\\d{10}',       # Matches +91-XXXXXXXXXX and +91 XXXXXXXXXX\n","        r'\\d{3}[-\\s]?\\d{3}[-\\s]?\\d{4}',  # Matches XXX-XXX-XXXX\n","        r'\\b\\d{10}\\b'              # Matches 10-digit numbers\n","    ]\n","\n","    for pattern in phone_patterns:\n","        matches = re.findall(pattern, text)\n","        if matches:\n","            phone_numbers = matches  # Store only the first successful match\n","            break  # Stop checking lower-priority patterns\n","\n","    return dob, phone_numbers\n","\n","# Process files in uploaded order\n","uploaded_file_order = list(uploaded_files.keys())  # Maintains order of uploaded files\n","dob_phone_results = {}\n","\n","for filename in uploaded_file_order:\n","    file_path = os.path.join(upload_dir, filename)\n","    text_content = \"\"\n","\n","    if filename.endswith(\".pdf\"):\n","        text_content = read_pdf(file_path)\n","    elif filename.endswith(\".docx\"):\n","        text_content = read_docx(file_path)\n","    else:\n","        continue  # Skip unsupported files\n","\n","    dob, phone_numbers = label_dob_phone(text_content)\n","    dob_phone_results[filename] = {\"Date of Birth\": dob, \"Phone Numbers\": phone_numbers}\n","\n","# Display extracted DOB & Phone Numbers\n","for file, data in dob_phone_results.items():\n","    print(f\"\\n--- {file} ---\")\n","    # Leave blank if no DOB is found\n","    print(f\"Date of Birth: {data['Date of Birth'] if data['Date of Birth'] else ''}\")\n","    # Leave blank if no Phone Numbers are found\n","    print(f\"Phone Numbers: {', '.join(data['Phone Numbers']) if data['Phone Numbers'] else ''}\")\n","    print(\"-\" * 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GEbGeZxkopks","executionInfo":{"status":"ok","timestamp":1742798524946,"user_tz":-330,"elapsed":4130,"user":{"displayName":"Suryansh Singh","userId":"04524882864171595882"}},"outputId":"5def100f-2016-45aa-871a-59e8a8aa1eef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Abhishek Raj.pdf ---\n","Date of Birth: \n","Phone Numbers: +91 6203706710\n","--------------------------------------------------------------------------------\n","\n","--- Naukri_PradeepKumar[3y_0m].pdf ---\n","Date of Birth: \n","Phone Numbers: +91 6394471537\n","--------------------------------------------------------------------------------\n","\n","--- Naukri_PrashantSinghal[8y_0m].pdf ---\n","Date of Birth: \n","Phone Numbers: 784-000-4096\n","--------------------------------------------------------------------------------\n","\n","--- Parth Rustagi.pdf ---\n","Date of Birth: 24-10-2003\n","Phone Numbers: +91-8950624410\n","--------------------------------------------------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"8YNHh-rEZ3en"},"source":["# **Educations and Experience**"]},{"cell_type":"markdown","metadata":{"id":"HTC3guM8Eau_"},"source":["**Cell 1** (normal data)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1504,"status":"ok","timestamp":1742886901146,"user":{"displayName":"Suryansh Singh","userId":"04524882864171595882"},"user_tz":-330},"id":"w8H10tu1Fit3","outputId":"e81381e9-d416-46f3-814b-63a6ac405959"},"outputs":[{"output_type":"stream","name":"stdout","text":["Skipping non-resume file: Naukri_SagarDas[2y_0m].doc\n","Skipping non-resume file: sample_data\n","\n","‚úÖ Arranged data saved to arranged data.json\n","[\n","    {\n","        \"File Name\": \"Suraj kumar.pdf\",\n","        \"Education\": {},\n","        \"Experience\": {}\n","    },\n","    {\n","        \"File Name\": \"Naukri_Yogeshbhati[3y_0m].docx\",\n","        \"Education\": {},\n","        \"Experience\": {}\n","    },\n","    {\n","        \"File Name\": \"Naukri_RajnishSingh[0y_6m].pdf\",\n","        \"Education\": {},\n","        \"Experience\": {}\n","    },\n","    {\n","        \"File Name\": \"Naukri_Deepak[3y_0m].pdf\",\n","        \"Education\": {\n","            \"Education 1\": {\n","                \"Degree\": \"B. Com\",\n","                \"University/Board\": \"MAHGU\",\n","                \"Year\": 2020,\n","                \"Percentage\": 65.0\n","            },\n","            \"Education 2\": {\n","                \"Degree\": \"12th\",\n","                \"University/Board\": \"Haryana Board\",\n","                \"Year\": 2013,\n","                \"Percentage\": 60.0\n","            },\n","            \"Education 3\": {\n","                \"Degree\": \"10th\",\n","                \"University/Board\": \"Haryana Board\",\n","                \"Year\": 2010,\n","                \"Percentage\": 63.0\n","            }\n","        },\n","        \"Experience\": {\n","            \"Experience 1\": {\n","                \"Company\": \"Imigious Technologies PVT LTD\",\n","                \"Role\": \"Account Executive\",\n","                \"Start Date\": \"Jan 2024\",\n","                \"End Date\": \"Till Date\"\n","            }\n","        }\n","    },\n","    {\n","        \"File Name\": \"Parth Rustagi.pdf\",\n","        \"Education\": {},\n","        \"Experience\": {}\n","    },\n","    {\n","        \"File Name\": \"Naukri_ShivNarayan[1y_0m].docx\",\n","        \"Education\": {},\n","        \"Experience\": {}\n","    },\n","    {\n","        \"File Name\": \"Prashant Yadav.pdf\",\n","        \"Education\": {},\n","        \"Experience\": {}\n","    },\n","    {\n","        \"File Name\": \"Naukri_ganeshbora[1y_8m].docx\",\n","        \"Education\": {},\n","        \"Experience\": {}\n","    }\n","]\n"]}],"source":["import pdfplumber\n","import docx\n","import re\n","import json\n","import os\n","import glob\n","\n","# Define the target directory\n","upload_dir = \"/content/\"\n","\n","# Load existing JSON data if available\n","json_filename = \"multiple_resumes_data.json\"\n","try:\n","    with open(json_filename, \"r\") as json_file:\n","        resume_data = json.load(json_file)\n","except FileNotFoundError:\n","    resume_data = []  # If file doesn't exist, create an empty list\n","\n","# Convert existing data to a dictionary for quick lookup\n","resume_dict = {entry[\"File Name\"]: entry for entry in resume_data}\n","\n","# Function to normalize file names (remove suffixes like (7))\n","def normalize_file_name(file_name):\n","    # Remove suffixes like (7), (8), etc.\n","    return re.sub(r\"\\s\\(\\d+\\)(?=\\.\\w+$)\", \"\", file_name)\n","\n","# Function to extract text from a PDF\n","def extract_text_from_pdf(pdf_path):\n","    text = \"\"\n","    with pdfplumber.open(pdf_path) as pdf:\n","        for page in pdf.pages:\n","            text += page.extract_text() + \"\\n\"\n","    return text.strip()\n","\n","# Function to extract text from a DOCX\n","def extract_text_from_docx(docx_path):\n","    doc = docx.Document(docx_path)\n","    text = \"\\n\".join([para.text for para in doc.paragraphs])\n","    return text.strip()\n","\n","# Function to extract education details\n","def extract_education(text):\n","    education_pattern = re.findall(r\"Passed\\s(.+?)\\sfrom\\s(.+?)\\sin\\s(\\d{4})\\swith\\s([\\d.]+)%?\", text, re.IGNORECASE)\n","    education_list = []\n","    for edu in education_pattern:\n","        education_list.append({\n","            \"Degree\": edu[0].strip(),\n","            \"University/Board\": edu[1].strip(),\n","            \"Year\": int(edu[2]),\n","            \"Percentage\": float(edu[3]) if edu[3] else None\n","        })\n","    return education_list\n","\n","# Function to extract experience details\n","def extract_experience(text):\n","    experience_pattern = re.findall(r\"Working in\\s(.+?)\\sas\\s(.+?)\\sfrom\\s(.+?)\\sto\\s(Till Date|\\d{4})\", text, re.IGNORECASE)\n","    experience_list = []\n","    for exp in experience_pattern:\n","        experience_list.append({\n","            \"Company\": exp[0].strip(),\n","            \"Role\": exp[1].strip(),\n","            \"Start Date\": exp[2].strip(),\n","            \"End Date\": exp[3].strip()\n","        })\n","    return experience_list\n","\n","# Process all files in the /content/ directory\n","arranged_data = {}\n","for file_path in glob.glob(os.path.join(upload_dir, \"*\")):\n","    file_name = os.path.basename(file_path)\n","\n","    # Skip non-resume files\n","    if not file_name.endswith(('.pdf', '.docx')):\n","        print(f\"Skipping non-resume file: {file_name}\")\n","        continue\n","\n","    # Normalize the file name (remove suffixes like (7))\n","    normalized_file_name = normalize_file_name(file_name)\n","\n","    # Identify file type and extract text\n","    if file_name.endswith(\".pdf\"):\n","        resume_text = extract_text_from_pdf(file_path)\n","    elif file_name.endswith(\".docx\"):\n","        resume_text = extract_text_from_docx(file_path)\n","    else:\n","        print(f\"Skipping unsupported file: {file_name}\")\n","        continue\n","\n","    # Extract education and experience\n","    education_data = extract_education(resume_text)\n","    experience_data = extract_experience(resume_text)\n","\n","    # Convert Education and Experience to the desired structure\n","    education_data = {\n","        f\"Education {i+1}\": edu for i, edu in enumerate(education_data)\n","    }\n","\n","    experience_data = {\n","        f\"Experience {i+1}\": exp for i, exp in enumerate(experience_data)\n","    }\n","\n","    # Create new entry (without \"Extracted Text\")\n","    new_entry = {\n","        \"File Name\": normalized_file_name,\n","        \"Education\": education_data,\n","        \"Experience\": experience_data\n","    }\n","\n","    # Store in arranged_data dictionary\n","    arranged_data[normalized_file_name] = new_entry\n","\n","    # Replace the existing entry if the same file name exists, otherwise add a new entry\n","    resume_dict[normalized_file_name] = new_entry\n","\n","# Convert dictionary back to list\n","resume_data = list(resume_dict.values())\n","\n","# Save extracted data as JSON\n","with open(json_filename, \"w\") as json_file:\n","    json.dump(resume_data, json_file, indent=4)\n","\n","# Save arranged data to JSON file\n","with open('/content/arranged data.json', 'w') as f:\n","    json.dump(arranged_data, f, indent=4)\n","\n","print(\"\\n‚úÖ Arranged data saved to arranged data.json\")\n","\n","# Display JSON Output\n","print(json.dumps(resume_data, indent=4))"]},{"cell_type":"markdown","source":["# **Test**"],"metadata":{"id":"32hPyDMQay0L"}},{"cell_type":"code","source":["# Install required packages\n","!pip install pyresparser\n","!pip install python-docx\n","!pip install PyPDF2\n","\n","import os\n","from pyresparser import ResumeParser\n","import docx\n","import PyPDF2\n","from pathlib import Path\n","\n","def extract_text_from_pdf(file_path):\n","    \"\"\"Extract text from PDF file\"\"\"\n","    text = \"\"\n","    try:\n","        with open(file_path, 'rb') as file:\n","            pdf_reader = PyPDF2.PdfReader(file)\n","            for page in pdf_reader.pages:\n","                text += page.extract_text()\n","    except Exception as e:\n","        print(f\"Error reading PDF: {e}\")\n","    return text\n","\n","def extract_text_from_docx(file_path):\n","    \"\"\"Extract text from DOCX file\"\"\"\n","    text = \"\"\n","    try:\n","        doc = docx.Document(file_path)\n","        for para in doc.paragraphs:\n","            text += para.text + \"\\n\"\n","    except Exception as e:\n","        print(f\"Error reading DOCX: {e}\")\n","    return text\n","\n","def process_resume(file_path):\n","    \"\"\"Process resume file and extract information\"\"\"\n","    file_extension = Path(file_path).suffix.lower()\n","\n","    # Extract text based on file type\n","    if file_extension == '.pdf':\n","        text = extract_text_from_pdf(file_path)\n","    elif file_extension == '.docx':\n","        text = extract_text_from_docx(file_path)\n","    else:\n","        print(f\"Unsupported file format: {file_extension}\")\n","        return None\n","\n","    try:\n","        # Parse resume using pyresparser\n","        data = ResumeParser(file_path).get_extracted_data()\n","\n","        # Extract required fields\n","        extracted_data = {\n","            'name': data.get('name', 'Not found'),\n","            'email': data.get('email', 'Not found'),\n","            'mobile_number': data.get('mobile_number', 'Not found'),\n","            'skills': data.get('skills', []),\n","            'total_experience': data.get('total_experience', 'Not found'),\n","            'college_name': data.get('college', 'Not found'),\n","            'degree': data.get('degree', 'Not found'),\n","            'designation': data.get('designation', 'Not found'),\n","            'company_names': data.get('company_names', [])\n","        }\n","\n","        return extracted_data\n","\n","    except Exception as e:\n","        print(f\"Error parsing resume: {e}\")\n","        return None\n","\n","def main():\n","    # Directory path in Google Colab\n","    directory = '/content/'\n","\n","    # Supported file extensions\n","    supported_extensions = ('.pdf', '.docx')\n","\n","    # Process all resume files in directory\n","    for filename in os.listdir(directory):\n","        if filename.lower().endswith(supported_extensions):\n","            file_path = os.path.join(directory, filename)\n","            print(f\"\\nProcessing: {filename}\")\n","            print(\"-\" * 50)\n","\n","            # Extract data\n","            result = process_resume(file_path)\n","\n","            if result:\n","                # Print extracted information\n","                print(f\"Name: {result['name']}\")\n","                print(f\"Email: {result['email']}\")\n","                print(f\"Mobile Number: {result['mobile_number']}\")\n","                print(\"Skills:\", \", \".join(result['skills']) if result['skills'] else \"Not found\")\n","                print(f\"Total Experience: {result['total_experience']}\")\n","                print(f\"College Name: {result['college_name']}\")\n","                print(f\"Degree: {result['degree']}\")\n","                print(f\"Designation: {result['designation']}\")\n","                print(\"Company Names:\", \", \".join(result['company_names']) if result['company_names'] else \"Not found\")\n","            else:\n","                print(\"Failed to process resume\")\n","            print(\"-\" * 50)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LsnbSXiwv_Eq","executionInfo":{"status":"ok","timestamp":1742902681727,"user_tz":-330,"elapsed":33510,"user":{"displayName":"Suryansh Singh","userId":"04524882864171595882"}},"outputId":"7a215da6-6bfe-44fa-823b-af0b07a3ef2c"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyresparser\n","  Downloading pyresparser-1.0.6-py3-none-any.whl.metadata (7.4 kB)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (25.3.0)\n","Requirement already satisfied: blis>=0.2.4 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (1.2.0)\n","Requirement already satisfied: certifi>=2019.6.16 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (2025.1.31)\n","Requirement already satisfied: chardet>=3.0.4 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (5.2.0)\n","Requirement already satisfied: cymem>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (2.0.11)\n","Requirement already satisfied: docx2txt>=0.7 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (0.9)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (3.10)\n","Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (4.23.0)\n","Requirement already satisfied: nltk>=3.4.3 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (3.9.1)\n","Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (2.0.2)\n","Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (2.2.2)\n","Requirement already satisfied: pdfminer.six>=20181108 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (20231228)\n","Requirement already satisfied: preshed>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (3.0.9)\n","Collecting pycryptodome>=3.8.2 (from pyresparser)\n","  Downloading pycryptodome-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n","Collecting pyrsistent>=0.15.2 (from pyresparser)\n","  Downloading pyrsistent-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n","Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (2.8.2)\n","Requirement already satisfied: pytz>=2019.1 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (2025.1)\n","Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (2.32.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (1.17.0)\n","Requirement already satisfied: sortedcontainers>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (2.4.0)\n","Requirement already satisfied: spacy>=2.1.4 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (3.8.4)\n","Requirement already satisfied: srsly>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (2.5.1)\n","Requirement already satisfied: thinc>=7.0.4 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (8.3.4)\n","Requirement already satisfied: tqdm>=4.32.2 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (4.67.1)\n","Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (2.3.0)\n","Requirement already satisfied: wasabi>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from pyresparser) (1.1.3)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0.1->pyresparser) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0.1->pyresparser) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0.1->pyresparser) (0.23.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.4.3->pyresparser) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.4.3->pyresparser) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.4.3->pyresparser) (2024.11.6)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.2->pyresparser) (2025.1)\n","Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six>=20181108->pyresparser) (3.4.1)\n","Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six>=20181108->pyresparser) (43.0.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from preshed>=2.0.1->pyresparser) (1.0.12)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.1.4->pyresparser) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.1.4->pyresparser) (1.0.5)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.1.4->pyresparser) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.1.4->pyresparser) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.1.4->pyresparser) (0.15.2)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.1.4->pyresparser) (2.10.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.1.4->pyresparser) (3.1.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy>=2.1.4->pyresparser) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.1.4->pyresparser) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=2.1.4->pyresparser) (3.5.0)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc>=7.0.4->pyresparser) (0.1.5)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six>=20181108->pyresparser) (1.17.1)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=2.1.4->pyresparser) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.1.4->pyresparser) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.1.4->pyresparser) (2.27.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.1.4->pyresparser) (4.12.2)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2.1.4->pyresparser) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=2.1.4->pyresparser) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.1.4->pyresparser) (0.21.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=2.1.4->pyresparser) (7.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy>=2.1.4->pyresparser) (3.0.2)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six>=20181108->pyresparser) (2.22)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=2.1.4->pyresparser) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.1.4->pyresparser) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.1.4->pyresparser) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=2.1.4->pyresparser) (1.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=2.1.4->pyresparser) (0.1.2)\n","Downloading pyresparser-1.0.6-py3-none-any.whl (4.2 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pycryptodome-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyrsistent-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (120 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyrsistent, pycryptodome, pyresparser\n","Successfully installed pycryptodome-3.22.0 pyresparser-1.0.6 pyrsistent-0.20.0\n","Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n","Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.1)\n","Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.12.2)\n","Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n","\n","Processing: Suraj kumar.pdf\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/spacy/util.py:918: UserWarning: [W094] Model 'en_training' (0.0.0) specifies an under-constrained spaCy version requirement: >=2.1.4. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the \"spacy_version\" in your meta.json to a version range, with a lower and upper pin. For example: >=3.8.4,<3.9.0\n","  warnings.warn(warn_msg)\n"]},{"output_type":"stream","name":"stdout","text":["Error parsing resume: [E053] Could not read config file from /usr/local/lib/python3.11/dist-packages/pyresparser/config.cfg\n","Failed to process resume\n","--------------------------------------------------\n","\n","Processing: Naukri_Yogeshbhati[3y_0m].docx\n","--------------------------------------------------\n","Error parsing resume: [E053] Could not read config file from /usr/local/lib/python3.11/dist-packages/pyresparser/config.cfg\n","Failed to process resume\n","--------------------------------------------------\n","\n","Processing: Naukri_RajnishSingh[0y_6m].pdf\n","--------------------------------------------------\n","Error parsing resume: [E053] Could not read config file from /usr/local/lib/python3.11/dist-packages/pyresparser/config.cfg\n","Failed to process resume\n","--------------------------------------------------\n","\n","Processing: Naukri_Deepak[3y_0m].pdf\n","--------------------------------------------------\n","Error parsing resume: [E053] Could not read config file from /usr/local/lib/python3.11/dist-packages/pyresparser/config.cfg\n","Failed to process resume\n","--------------------------------------------------\n","\n","Processing: Parth Rustagi.pdf\n","--------------------------------------------------\n","Error parsing resume: [E053] Could not read config file from /usr/local/lib/python3.11/dist-packages/pyresparser/config.cfg\n","Failed to process resume\n","--------------------------------------------------\n","\n","Processing: Naukri_ShivNarayan[1y_0m].docx\n","--------------------------------------------------\n","Error parsing resume: [E053] Could not read config file from /usr/local/lib/python3.11/dist-packages/pyresparser/config.cfg\n","Failed to process resume\n","--------------------------------------------------\n","\n","Processing: Prashant Yadav.pdf\n","--------------------------------------------------\n","Error parsing resume: [E053] Could not read config file from /usr/local/lib/python3.11/dist-packages/pyresparser/config.cfg\n","Failed to process resume\n","--------------------------------------------------\n","\n","Processing: Naukri_ganeshbora[1y_8m].docx\n","--------------------------------------------------\n","Error parsing resume: [E053] Could not read config file from /usr/local/lib/python3.11/dist-packages/pyresparser/config.cfg\n","Failed to process resume\n","--------------------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"P4gl2vRzExxY"},"source":["# **Cell 2 (Eduation, Experience and Skills)**"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":718,"status":"ok","timestamp":1742893457099,"user":{"displayName":"Suryansh Singh","userId":"04524882864171595882"},"user_tz":-330},"id":"gBz8Bry05Apm","outputId":"f9ae6536-9330-4bfa-ee4b-e4df492f26d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 8 resume files in /content/\n","Processing: Suraj kumar.pdf\n","Processing: Naukri_Yogeshbhati[3y_0m].docx\n","Processing: Naukri_RajnishSingh[0y_6m].pdf\n","Processing: Naukri_Deepak[3y_0m].pdf\n","Processing: Parth Rustagi.pdf\n","Processing: Naukri_ShivNarayan[1y_0m].docx\n","Processing: Prashant Yadav.pdf\n","Processing: Naukri_ganeshbora[1y_8m].docx\n","\n","‚úÖ Parsed 8 resumes.\n","\n","============================================================\n","üìÑ RESUME: Suraj kumar.pdf\n","============================================================\n","\n","üìö RAW EDUCATION:\n","EDUCATION 1 Mar 2022 - Sep\n","\n","üíº RAW EXPERIENCE:\n","EXPERIENCE 1 experience\n","EXPERIENCE 2 2 Years 6 Months\n","EXPERIENCE 3 KEY\n","EXPERIENCE 4 experience in Account Payable,\n","EXPERIENCE 5 Invoice Processing, Vendor Payment Processing, Vendor Reconciliation, Bank\n","EXPERIENCE 6 Reconciliation, Vendor Query Resolution, GST,TDS, Vendor Master Data\n","EXPERIENCE 7 Creation and advance excel Power BI, SQL etc.. I have also good knowledge of\n","\n","üîß SKILLS:\n","SKILL 1 excel\n","SKILL 2 sap\n","\n","\n","\n","============================================================\n","üìÑ RESUME: Naukri_Yogeshbhati[3y_0m].docx\n","============================================================\n","\n","üìö RAW EDUCATION:\n","\n","üíº RAW EXPERIENCE:\n","EXPERIENCE 1 professionally along the corporate leader while contributing effectively to the organization.\n","EXPERIENCE 2 Ability to work in team.\n","EXPERIENCE 3 Positive Attitude, Analytical\n","EXPERIENCE 4 Experience:\n","EXPERIENCE 5 Presently Working with Oximus technologies Pvt Ltd as an Account Exe from July-2021.\n","EXPERIENCE 6 Job Responsibilities\n","EXPERIENCE 7 Verify and validate the vendor invoices.\n","\n","üîß SKILLS:\n","SKILL 1 knowledge of  word\n","SKILL 2 excel\n","SKILL 3 word\n","SKILL 4 sap\n","\n","\n","\n","============================================================\n","üìÑ RESUME: Naukri_RajnishSingh[0y_6m].pdf\n","============================================================\n","\n","üìö RAW EDUCATION:\n","EDUCATION 1 Assets:\n","EDUCATION 2 Skills:\n","EDUCATION 3 ÔÅ∂ Sky2C  Freight  System  Inc.ÔÄ†\n","EDUCATION 4 ÔÉò Accounts  Payable  Department\n","EDUCATION 5 ÔÇ∑ Vendor  ledger  Reconcile\n","\n","üíº RAW EXPERIENCE:\n","EXPERIENCE 1 Professional as\n","EXPERIENCE 2 well as personal growth while being resourceful, innovative, and flexible.\n","EXPERIENCE 3 EXAMINATION\n","EXPERIENCE 4 SCHOOL/ COLLEGE\n","EXPERIENCE 5 BOARD\n","EXPERIENCE 6 YEAR OF\n","EXPERIENCE 7 PASSING\n","\n","üîß SKILLS:\n","SKILL 1 ai\n","SKILL 2 communication\n","SKILL 3 learning\n","SKILL 4 excel\n","SKILL 5 word\n","SKILL 6 tally\n","\n","\n","\n","============================================================\n","üìÑ RESUME: Naukri_Deepak[3y_0m].pdf\n","============================================================\n","\n","üìö RAW EDUCATION:\n","EDUCATION 1 ÔÇ∑ Passed  B. Com  from  MAHGU  in 2020  with  65.00%\n","EDUCATION 2 ÔÇ∑ Passed  12th from  Haryana  Board  in 201 3 with  60.00 %\n","EDUCATION 3 ÔÇ∑ Passed  10th from  Haryana  Board  in 2010  with  63.00 %\n","\n","üíº RAW EXPERIENCE:\n","EXPERIENCE 1 Professional who aspires to work in challenging environment where my qualification\n","EXPERIENCE 2 and my\n","EXPERIENCE 3 Experience\n","EXPERIENCE 4 ÔÇ∑ Working in Imigious Technologies PVT LTD as Account Executive from Jan 2024 to Till Date.\n","EXPERIENCE 5 ÔÇ∑ 2 Year Experience in Indi cation Instrument LTD as Account Executive.\n","EXPERIENCE 6 Key Responsibility Area &\n","\n","üîß SKILLS:\n","SKILL 1 ÔÇ∑  office ( word\n","SKILL 2 rest\n","SKILL 3 excel\n","SKILL 4 word\n","SKILL 5 office\n","SKILL 6 ÔÇ∑ sap operation\n","SKILL 7 sap\n","\n","\n","\n","============================================================\n","üìÑ RESUME: Parth Rustagi.pdf\n","============================================================\n","\n","üìö RAW EDUCATION:\n","EDUCATION 1 ÔÇ∑ Pursuing Bachelor of Commerce from Delhi University.\n","EDUCATION 2 ÔÇ∑ Completed Online Certifications from CFI (Corporate Finance Institute) in\n","EDUCATION 3 1. Reading Financial Statements\n","EDUCATION 4 XIIth CBSE Board(2021) 84%\n","EDUCATION 5 Xth CBSE Board(2019) 81%\n","EDUCATION 6 E-Mail : parthrustagi2410@gmail.com\n","EDUCATION 7 CORE COMPETENCIES\n","EDUCATION 8 ‚óè Self motivated, Adaptable\n","EDUCATION 9 ‚óè Problem-solving and organizational\n","\n","üíº RAW EXPERIENCE:\n","EXPERIENCE 1 EMPLOYMENT RECITAL\n","EXPERIENCE 2 ÔÄ†ÔÄ†ÔÄ†ÔÉúÔÄ† Presently Working in PRECISE LOGISTIC SOLUTION PRIVATE LIMITED from Feb 2024 and\n","EXPERIENCE 3 work profile includes :-\n","EXPERIENCE 4 ÔÇ∑ GST Returns( GSTR-1 ,GSTR-3B)\n","EXPERIENCE 5 ÔÇ∑ ITC Reconciliation from GSTR-2B vis a vis books.\n","EXPERIENCE 6 ÔÇ∑ Responsible for preparation of TDS (Payment & Returns).\n","EXPERIENCE 7 ÔÇ∑ Journal Posting.\n","\n","üîß SKILLS:\n","SKILL 1 excel\n","SKILL 2 ui\n","SKILL 3 o  excel ( vlookup\n","SKILL 4 tally\n","\n","\n","\n","============================================================\n","üìÑ RESUME: Naukri_ShivNarayan[1y_0m].docx\n","============================================================\n","\n","üìö RAW EDUCATION:\n","\n","üíº RAW EXPERIENCE:\n","\n","üîß SKILLS:\n","\n","\n","\n","============================================================\n","üìÑ RESUME: Prashant Yadav.pdf\n","============================================================\n","\n","üìö RAW EDUCATION:\n","\n","üíº RAW EXPERIENCE:\n","EXPERIENCE 1 EXPERIENCE\n","EXPERIENCE 2 TECHNICAL\n","\n","üîß SKILLS:\n","SKILL 1 ai\n","SKILL 2 leadership\n","SKILL 3 communication\n","SKILL 4 ui\n","SKILL 5 erp\n","SKILL 6 tally\n","\n","\n","\n","============================================================\n","üìÑ RESUME: Naukri_ganeshbora[1y_8m].docx\n","============================================================\n","\n","üìö RAW EDUCATION:\n","EDUCATION 1 Bachelor in Commerce\n","EDUCATION 2 [Delhi University]\n","EDUCATION 3 Diploma in E-Accounting: - Finance, Taxation, Auditing.\n","\n","üíº RAW EXPERIENCE:\n","EXPERIENCE 1 experience on legal aspects while working with experienced person.\n","EXPERIENCE 2 Professional Experience:-\n","EXPERIENCE 3 Audit Assistant\n","EXPERIENCE 4 [Raas and Associates]\n","EXPERIENCE 5 [Jangpura,Delhji]\n","EXPERIENCE 6 [27-Feb-2023 To 19-June-2023].\n","EXPERIENCE 7 Verifying financial statements, including balance sheets, income statements, and cash flow statements\n","\n","üîß SKILLS:\n","SKILL 1 ai\n","SKILL 2 communication\n","SKILL 3 excel\n","SKILL 4 erp\n","SKILL 5 tally\n","SKILL 6 tally prime)\n","\n","\n","\n","‚úÖ Raw formatted data saved to raw data.json\n"]}],"source":["import os\n","import glob\n","import json\n","import re\n","from datetime import datetime\n","import PyPDF2\n","import docx\n","import nltk\n","\n","# Ensure NLTK resources are downloaded properly\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","except LookupError:\n","    nltk.download('punkt')\n","\n","try:\n","    nltk.data.find('corpora/stopwords')\n","except LookupError:\n","    nltk.download('stopwords')\n","\n","try:\n","    nltk.data.find('taggers/averaged_perceptron_tagger')\n","except LookupError:\n","    nltk.download('averaged_perceptron_tagger')\n","\n","# Import NLTK modules after downloading resources\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","\n","# Define the target directory\n","upload_dir = \"/content/\"\n","\n","# List of files/folders to keep (other than resumes)\n","keep_files = {\".config\", \"extracted_resume_data.json\", \"sample_data\"}\n","\n","# üîπ Find all resume files in the directory\n","resume_files = []\n","for file_path in glob.glob(os.path.join(upload_dir, \"*\")):\n","    filename = os.path.basename(file_path)\n","\n","    if filename not in keep_files and filename.endswith(('.pdf', '.docx')):\n","        resume_files.append(filename)\n","\n","print(f\"Found {len(resume_files)} resume files in {upload_dir}\")\n","\n","# üîπ Process and Parse Resumes\n","resume_data = {}\n","\n","# Define regex patterns for education, experience, and skills\n","education_keywords = [\n","    r'education', r'academic background', r'qualification', r'degree',\n","    r'university', r'college', r'school', r'institute', r'b\\.?tech',\n","    r'b\\.?e', r'm\\.?tech', r'm\\.?e', r'phd', r'bachelor', r'master',\n","    r'diploma', r'certification'\n","]\n","\n","experience_keywords = [\n","    r'experience', r'employment', r'work history', r'career', r'job',\n","    r'intern', r'professional', r'position', r'role'\n","]\n","\n","skills_keywords = [\n","    r'skills', r'technical skills', r'key skills', r'proficiency',\n","    r'competencies', r'expertise', r'abilities', r'tools', r'languages'\n","]\n","\n","# Common degree abbreviations and terms\n","degree_terms = [\n","    'bachelor', 'master', 'phd', 'doctorate', 'bs', 'ba', 'b.s', 'b.a', 'ms', 'ma',\n","    'm.s', 'm.a', 'btech', 'b.tech', 'mtech', 'm.tech', 'be', 'b.e', 'me', 'm.e',\n","    'diploma', 'certificate', 'certification'\n","]\n","\n","# Common technical skills\n","common_skills = [\n","    'python', 'java', 'javascript', 'js', 'c++', 'c#', 'ruby', 'php', 'html', 'css','Tally',\n","    'typescript', 'sql', 'nosql', 'mongodb', 'mysql', 'postgresql', 'oracle', 'aws','ERP',\n","    'azure', 'gcp', 'git', 'docker', 'kubernetes', 'jenkins', 'jira', 'confluence','CRM',\n","    'react', 'angular', 'vue', 'node', 'express', 'django', 'flask', 'spring','Zoho Books',\n","    'bootstrap', 'jquery', 'redux', 'graphql', 'rest', 'api', 'json', 'xml','Quick Books',\n","    'machine learning', 'ml', 'ai', 'artificial intelligence', 'data science',\n","    'data analysis', 'big data', 'hadoop', 'spark', 'tableau', 'power bi',\n","    'excel', 'word', 'powerpoint', 'office', 'photoshop', 'illustrator',\n","    'figma', 'sketch', 'adobe', 'ui', 'ux', 'scrum', 'agile', 'waterfall','Sap',\n","    'project management', 'leadership', 'communication', 'teamwork', 'problem solving',\n","]\n","\n","def extract_text_from_pdf(pdf_path):\n","    try:\n","        with open(pdf_path, 'rb') as file:\n","            pdf_reader = PyPDF2.PdfReader(file)\n","            text = \"\"\n","            for page in pdf_reader.pages:\n","                text += page.extract_text() or \"\"  # Handle None returns\n","        return text\n","    except Exception as e:\n","        print(f\"Error extracting text from PDF {pdf_path}: {e}\")\n","        return \"\"\n","\n","def extract_text_from_docx(docx_path):\n","    try:\n","        doc = docx.Document(docx_path)\n","        text = \"\"\n","        for paragraph in doc.paragraphs:\n","            text += paragraph.text + \"\\n\"\n","        return text\n","    except Exception as e:\n","        print(f\"Error extracting text from DOCX {docx_path}: {e}\")\n","        return \"\"\n","\n","def extract_education(text):\n","    education_info = []\n","\n","    # First, find sections that might contain education information\n","    education_pattern = r'(?i)(?:education|academic|qualification|degree)(?:[\\s\\S]{0,1000}?)(?=experience|skills|projects|references|$)'\n","    education_sections = re.findall(education_pattern, text)\n","\n","    if not education_sections:\n","        # If no clear sections, look for specific degree patterns\n","        degree_pattern = r'(?i)(?:bachelor|master|phd|diploma|b\\.?tech|m\\.?tech|b\\.?e|m\\.?e|b\\.?a|m\\.?a|b\\.?s|m\\.?s)(?:[\\s\\S]{0,100}?)(?:university|college|institute|school)'\n","        degree_matches = re.findall(degree_pattern, text)\n","        for match in degree_matches:\n","            education_info.append(match.strip())\n","    else:\n","        # Process identified education sections\n","        for section in education_sections:\n","            # Look for degree information, institution names, and dates\n","            lines = section.split('\\n')\n","            for line in lines:\n","                line = line.strip()\n","                if line and any(re.search(f'(?i){term}', line) for term in degree_terms):\n","                    education_info.append(line)\n","                elif re.search(r'(?i)(?:university|college|institute|school)', line):\n","                    education_info.append(line)\n","\n","    # If still empty, try a more generic approach\n","    if not education_info:\n","        lines = text.split('\\n')\n","        for i, line in enumerate(lines):\n","            if re.search(r'(?i)education', line):\n","                # Get the next few lines after an \"Education\" header\n","                j = 1\n","                while i+j < len(lines) and j < 6:  # Look at up to 5 lines after the header\n","                    if lines[i+j].strip():\n","                        education_info.append(lines[i+j].strip())\n","                    j += 1\n","\n","    return education_info\n","\n","def extract_experience(text):\n","    experience_info = []\n","\n","    # Look for experience sections\n","    experience_pattern = r'(?i)(?:experience|work history|employment|professional)(?:[\\s\\S]{0,3000}?)(?=education|skills|projects|references|$)'\n","    experience_sections = re.findall(experience_pattern, text)\n","\n","    if experience_sections:\n","        for section in experience_sections:\n","            # Split into lines and process\n","            lines = section.split('\\n')\n","            for line in lines:\n","                line = line.strip()\n","                if line:  # Only add non-empty lines\n","                    experience_info.append(line)\n","\n","    # If nothing found with structured approach, extract paragraphs near experience keywords\n","    if not experience_info:\n","        for keyword in experience_keywords:\n","            pattern = rf'(?i)({keyword}[\\s\\S]{{0,500}}?(?:\\n\\n|\\Z))'\n","            matches = re.findall(pattern, text)\n","            for match in matches:\n","                if len(match) > 20:  # Ignore very short matches\n","                    experience_info.append(match.strip())\n","\n","    # Clean up experience entries\n","    cleaned_experience_info = []\n","    for entry in experience_info:\n","        # Remove extra spaces and newlines\n","        entry = re.sub(r'\\s+', ' ', entry).strip()\n","        if entry:  # Only add non-empty entries\n","            cleaned_experience_info.append(entry)\n","\n","    # Limit the number of experience entries to a maximum of 7\n","    return cleaned_experience_info[:7]\n","\n","def extract_skills(text):\n","    skills = set()\n","\n","    # Define filters for invalid skills\n","    invalid_patterns = [\n","        r'^\\d+$',  # Pure numbers (e.g., \"122016\")\n","        r'(?i)(sector|street|road|avenue|city|village|town|pin|zip|code)',  # Location-related terms\n","        r'(?i)(gurugram|grurgram|dundahera|delhi|mumbai|bangalore)',  # Specific place names\n","        r'(?i)(marital|status|gender|nationality|father|mother|name|mr\\.|mrs\\.|ms\\.)',  # Personal details\n","        r'(and|or|the|with|from|to|:)$',  # Common conjunctions/prepositions and colons\n","    ]\n","\n","    # First try to find the skills section\n","    skills_pattern = r'(?i)(?:skills|technical skills|key skills|proficiency|competencies)(?:[\\s\\S]{0,1000}?)(?=experience|education|projects|references|$)'\n","    skills_sections = re.findall(skills_pattern, text)\n","\n","    if skills_sections:\n","        for section in skills_sections:\n","            # Look for common skills directly (case-insensitive)\n","            for skill in common_skills:\n","                if skill.lower() in section.lower():\n","                    skills.add(skill.lower())\n","\n","            # Process bullet points or comma-separated items\n","            bullet_items = re.findall(r'(?:‚Ä¢|\\*|\\-|\\‚óã)([^‚Ä¢\\*\\-\\‚óã]+)', section)\n","            for item in bullet_items:\n","                item = item.strip().lower()\n","                if (len(item) > 1 and\n","                    len(item) < 30 and\n","                    not any(re.search(pattern, item) for pattern in invalid_patterns)):\n","                    # Check if it matches or is a substring of common_skills\n","                    if any(skill.lower() in item or item in skill.lower() for skill in common_skills):\n","                        skills.add(item)\n","\n","            # Check for comma-separated or \"&\"-separated items\n","            comma_lines = section.split('\\n')\n","            for line in comma_lines:\n","                if ',' in line or '&' in line:\n","                    items = re.split(r',|\\&', line)\n","                    for item in items:\n","                        item = item.strip().lower()\n","                        if (len(item) > 1 and\n","                            len(item) < 30 and\n","                            not any(re.search(pattern, item) for pattern in invalid_patterns)):\n","                            if any(skill.lower() in item or item in skill.lower() for skill in common_skills):\n","                                skills.add(item)\n","\n","    # If no skills section found, look for common skills in the entire text\n","    if not skills:\n","        for skill in common_skills:\n","            if skill.lower() in text.lower():\n","                skills.add(skill.lower())\n","\n","    # Clean up skills\n","    cleaned_skills = set()\n","    for skill in skills:\n","        cleaned_skill = re.sub(r'\\s+', ' ', skill).strip()\n","        cleaned_skill = re.sub(r'\\([^)]+\\)', '', cleaned_skill).strip()  # Remove parentheses\n","        cleaned_skill = re.sub(r'(ms|\\.|‚Ä¢|\\*)', '', cleaned_skill).strip()  # Remove \"ms\", dots, bullets\n","        if (cleaned_skill and\n","            not any(re.search(pattern, cleaned_skill) for pattern in invalid_patterns) and\n","            any(skill.lower() in cleaned_skill or cleaned_skill in skill.lower() for skill in common_skills)):\n","            cleaned_skills.add(cleaned_skill)\n","\n","    return list(cleaned_skills)\n","\n","# Process each resume file found in the directory\n","for filename in resume_files:\n","    file_path = os.path.join(upload_dir, filename)\n","    print(f\"Processing: {filename}\")\n","\n","    # Extract text based on file type\n","    if filename.endswith('.pdf'):\n","        resume_text = extract_text_from_pdf(file_path)\n","    elif filename.endswith('.docx'):\n","        resume_text = extract_text_from_docx(file_path)\n","    else:\n","        resume_text = \"\"\n","        print(f\"Unsupported file format: {filename}\")\n","        continue\n","\n","    # Parse resume for key information\n","    education = extract_education(resume_text)\n","    experience = extract_experience(resume_text)\n","    skills = extract_skills(resume_text)\n","\n","    # Store parsed data\n","    resume_data[filename] = {\n","        'education': education,\n","        'experience': experience,\n","        'skills': skills\n","    }\n","\n","# Removed the saving of extracted_resume_data.json\n","print(f\"\\n‚úÖ Parsed {len(resume_data)} resumes.\")\n","\n","# Display the extracted information with the updated format\n","for filename, data in resume_data.items():\n","    print(f\"\\n{'='*60}\\nüìÑ RESUME: {filename}\\n{'='*60}\")\n","\n","    print(\"\\nüìö RAW EDUCATION:\")\n","    for i, item in enumerate(data['education'], 1):\n","        print(f\"EDUCATION {i} {item}\")\n","\n","    print(\"\\nüíº RAW EXPERIENCE:\")\n","    for i, item in enumerate(data['experience'], 1):\n","        print(f\"EXPERIENCE {i} {item}\")\n","\n","    print(\"\\nüîß SKILLS:\")\n","    for i, item in enumerate(data['skills'], 1):\n","        print(f\"SKILL {i} {item}\")\n","\n","    print(\"\\n\")\n","\n","# Save raw formatted data to JSON file\n","with open('/content/raw data.json', 'w') as f:\n","    json.dump(resume_data, f, indent=2)\n","\n","print(f\"\\n‚úÖ Raw formatted data saved to raw data.json\")"]},{"cell_type":"code","source":["import json\n","import re\n","from datetime import datetime\n","\n","def extract_dates(data):\n","    \"\"\"\n","    Extract years and dates from education and experience sections\n","\n","    Args:\n","        data (dict): JSON data containing personal information\n","\n","    Returns:\n","        dict: Extracted dates for education and experience\n","    \"\"\"\n","    def find_years(text):\n","        \"\"\"\n","        Find years in text using different patterns\n","\n","        Args:\n","            text (str): Text to search for years\n","\n","        Returns:\n","            list: List of found years\n","        \"\"\"\n","        # Different regex patterns to match years\n","        year_patterns = [\n","            r'\\b(19\\d{2}|20\\d{2})\\b',  # Four-digit years from 1900-2099\n","            r'\\b(\\d{2})\\b',  # Two-digit years\n","        ]\n","\n","        years = []\n","        for pattern in year_patterns:\n","            years.extend(re.findall(pattern, str(text)))\n","\n","        # Convert two-digit years to four-digit years\n","        processed_years = []\n","        for year in years:\n","            if len(year) == 2:\n","                # Assume years 00-24 are 2000-2024, 25-99 are 1925-1999\n","                full_year = int(year)\n","                if 0 <= full_year <= 24:\n","                    full_year += 2000\n","                else:\n","                    full_year += 1900\n","                processed_years.append(str(full_year))\n","            else:\n","                processed_years.append(year)\n","\n","        return list(set(processed_years))\n","\n","    def extract_section_dates(section):\n","        \"\"\"\n","        Extract dates from a section of data\n","\n","        Args:\n","            section (list/dict): Section containing education or experience data\n","\n","        Returns:\n","            dict: Extracted dates from the section\n","        \"\"\"\n","        dates_info = {\n","            'years': [],\n","            'start_years': [],\n","            'end_years': []\n","        }\n","\n","        # Handle different data structures\n","        if isinstance(section, list):\n","            for item in section:\n","                item_years = find_years(item)\n","                dates_info['years'].extend(item_years)\n","        elif isinstance(section, dict):\n","            for key, value in section.items():\n","                item_years = find_years(value)\n","                dates_info['years'].extend(item_years)\n","\n","        # Sort and remove duplicates\n","        dates_info['years'] = sorted(list(set(dates_info['years'])))\n","\n","        return dates_info\n","\n","    # Extract dates from education and experience\n","    results = {\n","        'education': {},\n","        'experience': {}\n","    }\n","\n","    # Check and extract education dates\n","    if 'education' in data:\n","        results['education'] = extract_section_dates(data['education'])\n","\n","    # Check and extract experience dates\n","    if 'experience' in data:\n","        results['experience'] = extract_section_dates(data['experience'])\n","\n","    return results\n","\n","def process_json_file(file_path):\n","    \"\"\"\n","    Process JSON file and extract dates\n","\n","    Args:\n","        file_path (str): Path to the JSON file\n","\n","    Returns:\n","        dict: Extracted dates from the file\n","    \"\"\"\n","    try:\n","        with open(file_path, 'r') as file:\n","            data = json.load(file)\n","            return extract_dates(data)\n","    except FileNotFoundError:\n","        print(f\"Error: File {file_path} not found.\")\n","        return None\n","    except json.JSONDecodeError:\n","        print(f\"Error: Invalid JSON in {file_path}\")\n","        return None\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    # Replace with your actual file path\n","    file_path = 'raw data.json'\n","    extracted_dates = process_json_file(file_path)\n","\n","    if extracted_dates:\n","        print(\"Extracted Dates:\")\n","        print(json.dumps(extracted_dates, indent=2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h6y_iz5FiOjA","executionInfo":{"status":"ok","timestamp":1742893500318,"user_tz":-330,"elapsed":36,"user":{"displayName":"Suryansh Singh","userId":"04524882864171595882"}},"outputId":"123d5711-d078-40e7-a9c7-d6386c755c0b"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracted Dates:\n","{\n","  \"education\": {},\n","  \"experience\": {}\n","}\n"]}]},{"cell_type":"markdown","metadata":{"id":"144UfVw3zLuJ"},"source":["# **location labelling**"]},{"cell_type":"markdown","metadata":{"id":"la7RrG2tAiCy"},"source":[" **Cell 1 (first try)**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1225,"status":"ok","timestamp":1742798538155,"user":{"displayName":"Suryansh Singh","userId":"04524882864171595882"},"user_tz":-330},"id":"R5Nj92epLZIN","outputId":"c502ae72-6ed5-41b7-d57c-511f26d5f000"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing: /content/Abhishek Raj.pdf\n","{'file_name': 'Abhishek Raj.pdf', 'address': 'Abhishek Raj abhishekraj'}\n","\n","Processing: /content/Naukri_PrashantSinghal[8y_0m].pdf\n","{'file_name': 'Naukri_PrashantSinghal[8y_0m].pdf', 'address': 'Prashant Singhal PS Noida, India'}\n","\n","Processing: /content/Naukri_PradeepKumar[3y_0m].pdf\n","{'file_name': 'Naukri_PradeepKumar[3y_0m].pdf', 'error': 'Address not found in resume'}\n","\n","Processing: /content/Parth Rustagi.pdf\n","{'file_name': 'Parth Rustagi.pdf', 'error': 'Address not found in resume'}\n","\n"]}],"source":["import re\n","import pdfplumber\n","import docx\n","import os\n","\n","def extract_text_from_pdf(pdf_path):\n","    \"\"\"Extract text from a PDF.\"\"\"\n","    text = \"\"\n","    with pdfplumber.open(pdf_path) as pdf:\n","        for page in pdf.pages:\n","            extracted = page.extract_text()\n","            if extracted:\n","                text += extracted + \"\\n\"\n","    return text.strip()\n","\n","def extract_text_from_docx(docx_path):\n","    \"\"\"Extract text from a DOCX file.\"\"\"\n","    doc = docx.Document(docx_path)\n","    return \"\\n\".join(para.text for para in doc.paragraphs).strip()\n","\n","def clean_text(text):\n","    \"\"\"Remove excessive whitespace and join broken lines.\"\"\"\n","    return re.sub(r\"\\s+\", \" \", text).strip()\n","\n","def extract_address(text):\n","    \"\"\"Extract the full address from resume text using refined regex.\"\"\"\n","    address_patterns = [\n","        r\"Address[:\\-]?\\s*(?:Mobile[:\\-]?\\s*\\+?\\d{10,}[\\s,]*)?(.*?)(?=\\s*(?:\\d{6}|Mob\\.|Mobile|Phone|Email|DOB|Date of Birth|Objectives|Skills|Experience|Educational Qualification|Hobbies|Key Strength|Process|Training|Project|Payment|Status|Issues|Regulations|Invoices|Scenarios|Managing))\",\n","        r\"(House\\s*No\\s*\\d+\\s*.*?,\\s*.*?\\d{6})\",\n","        r\"(Vill\\s*[A-Za-z\\s]+,\\s*Tehsil\\s*[A-Za-z\\s]+,\\s*Distt\\.\\s*[A-Za-z\\s]+.*?\\d{6})\",\n","        r\"(Sector\\s*\\d+.*?,\\s*.*?\\d{6})\",\n","        r\"([A-Za-z0-9\\-\\/,.\\s]+?\\d{6})\",  # Generic address pattern with postal code\n","        r\"([A-Za-z0-9\\-\\/,.\\s]+Noida\\s*[-‚Äì]?\\s*\\d{6})\"  # Handles Noida-specific formatting\n","    ]\n","\n","    for pattern in address_patterns:\n","        match = re.search(pattern, text, re.IGNORECASE)\n","        if match:\n","            address = match.group(1).strip()\n","\n","            # Remove unwanted prefixes like \"Mobile : +91...\"\n","            address = re.sub(r\"^Mobile[:\\-]?\\s*\\+?\\d{10,}\\s*\", \"\", address).strip()\n","\n","            # Stop capturing after postal code or unwanted keywords\n","            address = re.split(r\"\\s*(?:\\d{6}|Mob\\.|Mobile|Phone|Email|DOB|Date of Birth|Objectives|Skills|Experience|Educational Qualification|Hobbies|Key Strength|Process|Training|Project|Payment|Status|Issues|Regulations|Invoices|Scenarios|Managing)\", address)[0].strip()\n","\n","            # Ensure it is a valid address (not a random word or short phrase)\n","            if len(address) > 10:  # Minimum length check\n","                return address\n","\n","    return None  # No valid address found\n","\n","# Directory containing uploaded files\n","upload_dir = \"/content/\"\n","resume_files = [f for f in os.listdir(upload_dir) if f.endswith((\".pdf\", \".docx\"))]\n","\n","# Process each resume file\n","extracted_data_8_1 = []  # ‚úÖ Renamed from extracted_data\n","for file in resume_files:\n","    file_path = os.path.join(upload_dir, file)\n","\n","    # Extract text based on file type\n","    if file.endswith(\".pdf\"):\n","        text = extract_text_from_pdf(file_path)\n","    elif file.endswith(\".docx\"):\n","        text = extract_text_from_docx(file_path)\n","    else:\n","        continue  # Skip non-resume files\n","\n","    text = clean_text(text)  # Clean extracted text\n","    address = extract_address(text)  # Extract address\n","\n","    # Store extracted results\n","    result = {\"file_name\": file}\n","    if address:\n","        result[\"address\"] = address\n","    else:\n","        result[\"error\"] = \"Address not found in resume\"\n","\n","    extracted_data_8_1.append(result)  # ‚úÖ Use new variable\n","    print(f\"Processing: {file_path}\\n{result}\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"0I4juKWeAgqK"},"source":["**Cell 2 (Cleaning)** (Residential Address)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1742798538690,"user":{"displayName":"Suryansh Singh","userId":"04524882864171595882"},"user_tz":-330},"id":"ld6u4FdvAfLh","outputId":"61f92c66-0773-4cae-cc13-920e6cd858ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["File: Abhishek Raj.pdf\n","Address: Abhishek Raj abhishekraj\n","\n","--------------------------------------------------------------------------------\n","File: Naukri_PrashantSinghal[8y_0m].pdf\n","Address: Prashant Singhal PS Noida, India\n","\n","--------------------------------------------------------------------------------\n"]}],"source":["import re\n","\n","# üìå Cell 8.2 - Process and Display Valid Addresses from Cell 8.1\n","\n","residential_address_results = {}  # Dictionary to store extracted residential addresses\n","\n","def clean_address(address):\n","    \"\"\"Remove contact numbers, email addresses, and unnecessary labels from extracted addresses.\"\"\"\n","    if not address:\n","        return \"\"\n","\n","    # Remove phone numbers\n","    address = re.sub(r\"\\b(?:Contact\\s*No|Mob|Mobile|Phone|Tel)\\s*[:\\-\\s]?\\s*\\+?\\d{7,15}\\b\", \"\", address, flags=re.IGNORECASE)\n","\n","    # Remove email addresses\n","    address = re.sub(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", \"\", address)\n","\n","    # Remove remaining unwanted labels (like \"Mail Id:\", \"Contact No:\")\n","    address = re.sub(r\"\\b(?:Mail\\s*Id|Email\\s*Id|E-mail\\s*Id)\\s*[:\\-]?\\s*\", \"\", address, flags=re.IGNORECASE)\n","\n","    return address.strip()\n","\n","def display_valid_addresses(extracted_data_8_1):\n","    \"\"\"Display only valid address entries from Cell 8.1, ignoring errors and cleaning addresses.\"\"\"\n","\n","    # Filter only entries with valid addresses\n","    valid_addresses = [entry for entry in extracted_data_8_1 if entry.get(\"address\")]\n","\n","    if not valid_addresses:\n","        print(\"No valid addresses found from Cell 8.1.\")\n","        return\n","\n","    # Process and display each valid address\n","    for entry in valid_addresses:\n","        cleaned_address = clean_address(entry['address'])  # Clean the address\n","        file_name = entry.get('file_name', 'Unknown')  # Handle missing file_name\n","\n","        # Store in dictionary\n","        residential_address_results[file_name] = cleaned_address\n","\n","        print(f\"File: {file_name}\")\n","        print(f\"Address: {cleaned_address}\\n\")\n","        print(\"-\" * 80)\n","\n","# ‚úÖ Call function using extracted_data_8_1\n","display_valid_addresses(extracted_data_8_1)\n"]},{"cell_type":"markdown","metadata":{"id":"y7IwCBfdXPro"},"source":["**Cell 3 (updation)** (Current Address)"]},{"cell_type":"code","source":["import os\n","import re\n","import pdfplumber\n","import docx\n","\n","# Initialize current_address_results to avoid NameError\n","current_address_results = {}\n","\n","# üìå **Expanded Library of Known Locations (Cities, States, Areas, Sectors)**\n","address_library = {\n","    \"Cities\": [\n","        # üîπ Metro & Tier-1 Cities\n","        \"Mumbai\", \"Delhi\", \"Bangalore\", \"Hyderabad\", \"Ahmedabad\", \"Chennai\", \"Kolkata\", \"Surat\", \"Pune\", \"Jaipur\",\n","        \"Lucknow\", \"Kanpur\", \"Nagpur\", \"Indore\", \"Thane\", \"Bhopal\", \"Visakhapatnam\", \"Patna\", \"Vadodara\",\n","        \"Ghaziabad\", \"Ludhiana\", \"Agra\", \"Nashik\", \"Faridabad\", \"Meerut\", \"Rajkot\", \"Varanasi\", \"Srinagar\",\n","        \"Aurangabad\", \"Dhanbad\", \"Amritsar\", \"Allahabad\", \"Ranchi\", \"Gwalior\", \"Jabalpur\", \"Coimbatore\",\n","        \"Vijayawada\", \"Jodhpur\", \"Madurai\", \"Raipur\", \"Kota\", \"Guwahati\", \"Chandigarh\", \"Solapur\", \"Mysore\",\n","        \"Gurgaon\", \"Gurugram\", \"Noida\", \"Jamshedpur\", \"Bhilai\", \"Cuttack\", \"Dehradun\", \"Kochi\", \"Navi Mumbai\", \"Howrah\",\n","\n","        # üîπ Emerging & Tier-2 Cities\n","        \"Salem\", \"Hubli\", \"Belgaum\", \"Tiruppur\", \"Mangalore\", \"Udaipur\", \"Ajmer\", \"Jamnagar\", \"Gandhinagar\",\n","        \"Thrissur\", \"Kollam\", \"Malappuram\", \"Kozhikode\", \"Bhavnagar\", \"Erode\", \"Vellore\", \"Siliguri\", \"Durgapur\",\n","        \"Asansol\", \"Bardhaman\", \"Muzaffarpur\", \"Gaya\", \"Bhagalpur\", \"Hajipur\", \"Bokaro\", \"Rohtak\", \"Panipat\",\n","        \"Sonipat\", \"Hisar\", \"Ambala\", \"Yamunanagar\", \"Saharanpur\", \"Aligarh\", \"Moradabad\", \"Bareilly\", \"Gorakhpur\",\n","        \"Jhansi\", \"Bilaspur\", \"Rewa\", \"Satna\", \"Ujjain\", \"Ratlam\", \"Bikaner\", \"Bhuj\", \"Anand\", \"Nadiad\", \"Mehsana\",\n","        \"Navsari\", \"Valsad\", \"Silvassa\", \"Daman\", \"Imphal\", \"Aizawl\", \"Itanagar\", \"Gangtok\", \"Shillong\",\n","        \"Dimapur\", \"Kohima\", \"Tezpur\", \"Jorhat\", \"Tinsukia\", \"Silchar\", \"Agartala\",\n","\n","        # üîπ Tier-3 & Growing Cities\n","        \"Karimnagar\", \"Warangal\", \"Nellore\", \"Tirupati\", \"Nanded\", \"Malegaon\", \"Sangli\", \"Akola\", \"Dhule\",\n","        \"Chhindwara\", \"Hoshangabad\", \"Betul\", \"Pithampur\", \"Sehore\", \"Vidisha\", \"Ratnagiri\", \"Kakinada\",\n","        \"Srikakulam\", \"Tadepalligudem\", \"Nizamabad\", \"Ongole\", \"Suryapet\", \"Bidar\", \"Gadag\", \"Bijapur\",\n","        \"Chitradurga\", \"Davangere\", \"Shimoga\", \"Tumkur\", \"Hassan\", \"Mandya\", \"Chikkamagaluru\", \"Kolar\",\n","        \"Nagercoil\", \"Thoothukudi\", \"Kumbakonam\", \"Erode\", \"Tirunelveli\", \"Karur\", \"Dindigul\", \"Thanjavur\",\n","        \"Pondicherry\", \"Yanam\", \"Port Blair\"\n","    ],\n","    \"States\": [\n","        # üîπ Indian States\n","        \"Andhra Pradesh\", \"Arunachal Pradesh\", \"Assam\", \"Bihar\", \"Chhattisgarh\", \"Goa\", \"Gujarat\", \"Haryana\",\n","        \"Himachal Pradesh\", \"Jharkhand\", \"Karnataka\", \"Kerala\", \"Madhya Pradesh\", \"Maharashtra\", \"Manipur\",\n","        \"Meghalaya\", \"Mizoram\", \"Nagaland\", \"Odisha\", \"Punjab\", \"Rajasthan\", \"Sikkim\", \"Tamil Nadu\",\n","        \"Telangana\", \"Tripura\", \"Uttar Pradesh\", \"Uttarakhand\", \"West Bengal\",\n","\n","        # üîπ Union Territories\n","        \"Delhi\", \"Puducherry\", \"Chandigarh\", \"Lakshadweep\", \"Andaman and Nicobar Islands\", \"Dadra and Nagar Haveli\",\n","        \"Daman and Diu\", \"Jammu and Kashmir\", \"Ladakh\"\n","    ],\n","    \"Sectors\": [f\"Sector {i}\" for i in range(1, 1001)],  # Covers Sector 1 to Sector 1000\n","    \"Other Areas\": [\n","        \"Phase\", \"Block\", \"Colony\", \"Road\", \"Street\", \"Avenue\", \"Distt\", \"Village\", \"Tehsil\", \"Town\", \"Ward\",\n","        \"Circle\", \"Residency\", \"Society\", \"Nagar\", \"Garden\", \"Vihar\", \"Enclave\", \"Lane\", \"Bypass\", \"Plaza\",\n","        \"Park\", \"Complex\", \"Market\", \"Gate\", \"Heights\", \"Crossing\", \"Square\", \"Tower\", \"Estate\", \"Boulevard\",\n","        \"Mahal\", \"Chowk\", \"Gali\", \"Marg\", \"Bazaar\", \"Peth\", \"Kurla\", \"Gunj\", \"Mandi\", \"Haat\", \"Pura\", \"Puram\",\n","        \"Basti\", \"Para\", \"Sarai\", \"Bagh\", \"Toli\", \"Faliya\", \"Vasahat\", \"Indira Colony\", \"Rajendra Nagar\",\n","        \"Ambedkar Nagar\", \"Shastri Nagar\", \"Jawahar Nagar\", \"Ashok Nagar\", \"Patel Nagar\", \"Nehru Nagar\",\n","        \"Lajpat Nagar\", \"Tagore Garden\", \"Krishna Nagar\", \"Lodhi Colony\", \"Rajouri Garden\", \"Green Park\",\n","        \"Defence Colony\", \"Sarvodaya Nagar\", \"Indiranagar\", \"Vivekanand Nagar\", \"Shivaji Park\", \"Mansarovar\",\n","        \"Keshav Puram\", \"Bhagat Singh Nagar\", \"Mahatma Gandhi Road\", \"Chandni Chowk\", \"Janpath\", \"MG Road\",\n","\n","        # üîπ New Address Types\n","        \"Industrial Area\", \"Tech Park\", \"SEZ\", \"IT Hub\", \"Corporate Park\", \"Software Park\", \"Business District\",\n","        \"City Center\", \"Logistics Park\", \"Special Economic Zone\", \"Metro Zone\", \"Ring Road\", \"Expressway\",\n","        \"Outer Ring Road\", \"Highway\", \"Freeway\", \"Flyover\", \"Suburb\", \"Urban Area\", \"Downtown\", \"CBD\",\n","        \"Gated Community\", \"Eco City\", \"Smart City\", \"Transport Nagar\", \"Warehouse Hub\"\n","    ]\n","}\n","\n","# üìå **Regex Patterns to Exclude College/University Names**\n","exclude_education_patterns = [\n","    r\"(?i)\\b(university|college|institute|academy|school|faculty|department|campus|board)\\b\",\n","    r\"(?i)\\b(B\\.?Tech|M\\.?Tech|MBA|B\\.?Sc|M\\.?Sc|B\\.?Com|M\\.?Com|PhD|BBA|BE|ME|Diploma|PGDM|UG|PG|LLB|LLM)\\b\",\n","    r\"(?i)\\b(engineering|management|science|commerce|arts|law|medicine|technology)\\b\",\n","]\n","compiled_exclude_patterns = [re.compile(pattern) for pattern in exclude_education_patterns]\n","\n","# üìå **Enhanced Address Matching Regex**\n","address_regex = r\"\\b([A-Za-z]+(?:\\s[A-Za-z]+)*)\\s*,?\\s*([A-Za-z]*)?\\s*(\\d{6})?\\b\"\n","\n","# üìå **Function to Extract Text from Resume**\n","def extract_text_from_resume(file_path):\n","    text = \"\"\n","    if file_path.endswith(\".pdf\"):\n","        with pdfplumber.open(file_path) as pdf:\n","            for page in pdf.pages:\n","                extracted = page.extract_text()\n","                if extracted:\n","                    text += extracted + \"\\n\"\n","    elif file_path.endswith(\".docx\"):\n","        doc = docx.Document(file_path)\n","        text = \"\\n\".join(para.text for para in doc.paragraphs)\n","    return text.strip()\n","\n","# üìå **Modified Function to Find Exact Word Matches from Library**\n","def match_address_with_library(text):\n","    found_matches = set()  # Using set to avoid duplicates\n","    words = re.findall(r\"\\b\\w+\\b\", text)  # Extract individual words\n","\n","    # Combine all library entries into one list for matching\n","    all_locations = (\n","        address_library[\"Cities\"] +\n","        address_library[\"States\"] +\n","        address_library[\"Sectors\"] +\n","        address_library[\"Other Areas\"]\n","    )\n","\n","    # Check each word against the library\n","    for word in words:\n","        if word in all_locations:\n","            found_matches.add(word)\n","\n","    return \", \".join(found_matches) if found_matches else \"\"\n","\n","# üìå **Processing Resumes from /content/**\n","upload_dir = \"/content/\"\n","resume_files = [f for f in os.listdir(upload_dir) if f.endswith((\".pdf\", \".docx\"))]\n","\n","# üìå **Extract and Match Addresses**\n","extracted_data = []\n","for file in resume_files:\n","    file_path = os.path.join(upload_dir, file)\n","    text = extract_text_from_resume(file_path)\n","    matched_address = match_address_with_library(text)\n","\n","    # Store extracted current address\n","    current_address_results[file] = matched_address if matched_address else \"\"\n","\n","    result = {\"file_name\": file, \"address\": matched_address if matched_address else \"\"}\n","    extracted_data.append(result)\n","\n","# üìå **Display Results in Required Format**\n","for result in extracted_data:\n","    print(f\"File: {result.get('file_name', 'Unknown')}\")\n","    print(f\"Address: {result.get('address', '')}\")  # Leaves blank if address is not found\n","    print(\"-\" * 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AZV6iR2CqzMB","executionInfo":{"status":"ok","timestamp":1742798542365,"user_tz":-330,"elapsed":1286,"user":{"displayName":"Suryansh Singh","userId":"04524882864171595882"}},"outputId":"43e691e0-7c74-476d-ff2e-ca46752d4673"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["File: Abhishek Raj.pdf\n","Address: Jamshedpur, Patna, Bihar, Bangalore\n","--------------------------------------------------------------------------------\n","File: Naukri_PrashantSinghal[8y_0m].pdf\n","Address: Noida\n","--------------------------------------------------------------------------------\n","File: Naukri_PradeepKumar[3y_0m].pdf\n","Address: Lucknow, Noida\n","--------------------------------------------------------------------------------\n","File: Parth Rustagi.pdf\n","Address: Haryana, Delhi\n","--------------------------------------------------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"r95B-KBeCzX4"},"source":["# **Arranging the data accordingly**"]},{"cell_type":"markdown","metadata":{"id":"hA7XYesAC68f"},"source":["**Name, Father name, Emial id, Gender, Date of Birth, Phone no., Educations, Experience, Residential Address and Current Address**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1742804576456,"user":{"displayName":"Suryansh Singh","userId":"04524882864171595882"},"user_tz":-330},"id":"k5Dvowlt4rhm","outputId":"13051bcb-58c6-49e9-cd26-e6217986effb"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================ Consolidated Resume Data ================\n","\n","\n","============================================================\n","üìÑ RESUME: Naukri_PrashantSinghal[8y_0m].pdf\n","============================================================\n","\n","First Name      : Prashant\n","Middle Name     : \n","Last Name       : Singhal\n","\n","Father's Name   : \n","\n","Gender          : \n","Languages       : Hindi, English\n","Email ID        : prashant.singhal9@gmail.com\n","\n","Date of Birth   : \n","Phone Numbers   : 784-000-4096\n","\n","Residential Address: Prashant Singhal PS Noida, India\n","\n","Current Address     : Noida\n","\n","============================================================\n","\n","============================================================\n","üìÑ RESUME: Parth Rustagi.pdf\n","============================================================\n","\n","First Name      : Parth\n","Middle Name     : \n","Last Name       : Rustagi\n","\n","Father's Name   : Mr. Gopal Dass\n","\n","Gender          : Male\n","Languages       : Hindi, English\n","Email ID        : parthrustagi2410@gmail.com\n","\n","Date of Birth   : 24-10-2003\n","Phone Numbers   : +91-8950624410\n","\n","Residential Address: \n","\n","Current Address     : Haryana, Delhi\n","\n","============================================================\n","\n","============================================================\n","üìÑ RESUME: Abhishek Raj.pdf\n","============================================================\n","\n","First Name      : Abhishek\n","Middle Name     : \n","Last Name       : Raj\n","\n","Father's Name   : \n","\n","Gender          : \n","Languages       : \n","Email ID        : abhishekraj270699@gmail.com\n","\n","Date of Birth   : \n","Phone Numbers   : +91 6203706710\n","\n","Residential Address: Abhishek Raj abhishekraj\n","\n","Current Address     : Jamshedpur, Patna, Bihar, Bangalore\n","\n","============================================================\n","\n","============================================================\n","üìÑ RESUME: Naukri_PradeepKumar[3y_0m].pdf\n","============================================================\n","\n","First Name      : Pradeep\n","Middle Name     : \n","Last Name       : Kumar\n","\n","Father's Name   : \n","\n","Gender          : \n","Languages       : \n","Email ID        : pradeep.info1720@gmail.com\n","\n","Date of Birth   : \n","Phone Numbers   : +91 6394471537\n","\n","Residential Address: \n","\n","Current Address     : Lucknow, Noida\n","\n","============================================================\n","\n","‚úÖ Personal data saved to personal data.json\n"]}],"source":["# üìå Consolidate results in a single output\n","print(\"\\n================ Consolidated Resume Data ================\\n\")\n","\n","# ‚úÖ Get a combined list of filenames from all result dictionaries\n","all_filenames = (\n","    set(name_results.keys()) |\n","    set(results.keys()) |\n","    set(dob_phone_results.keys()) |\n","    set(father_name_results.keys()) |\n","    set(residential_address_results.keys()) |\n","    set(current_address_results.keys()) |\n","    set(resume_data.keys())  # ‚úÖ Updated to use extracted resume data\n",")\n","\n","personal_data = {}\n","\n","for filename in all_filenames:\n","    print(f\"\\n{'='*60}\\nüìÑ RESUME: {filename}\\n{'='*60}\")\n","\n","    # Initialize data structure for this resume\n","    personal_data[filename] = {}\n","\n","    # üìå Helper function to format values\n","    def format_value(value):\n","        return value if value and value != \"None\" else \"\"\n","\n","    # üìå Display Name & Father's Name\n","    if filename in name_results:\n","        full_name = format_value(name_results[filename].get('Name', ''))\n","        name_parts = full_name.split()\n","\n","        first_name = name_parts[0] if len(name_parts) > 0 else \"\"\n","        middle_name = \" \".join(name_parts[1:-1]) if len(name_parts) > 2 else \"\"\n","        last_name = name_parts[-1] if len(name_parts) > 1 else \"\"\n","\n","        print(f\"\\nFirst Name      : {first_name}\")\n","        print(f\"Middle Name     : {middle_name}\")\n","        print(f\"Last Name       : {last_name}\")\n","\n","        # Add to personal_data\n","        personal_data[filename][\"First Name\"] = first_name\n","        personal_data[filename][\"Middle Name\"] = middle_name\n","        personal_data[filename][\"Last Name\"] = last_name\n","\n","    father_name = format_value(father_name_results.get(filename, ''))\n","    print(f\"\\nFather's Name   : {father_name}\")\n","    personal_data[filename][\"Father's Name\"] = father_name\n","\n","    # üìå Display Gender, Language, and Email ID\n","    if filename in results:\n","        gender = format_value(results[filename].get('Gender', ''))\n","        languages = ', '.join(results[filename].get('Languages', [])) if results[filename].get('Languages') else ''\n","        email = format_value(results[filename].get('Email ID', ''))\n","\n","        print(f\"\\nGender          : {gender}\")\n","        print(f\"Languages       : {languages}\")\n","        print(f\"Email ID        : {email}\")\n","\n","        # Add to personal_data\n","        personal_data[filename][\"Gender\"] = gender\n","        personal_data[filename][\"Languages\"] = languages\n","        personal_data[filename][\"Email ID\"] = email\n","\n","    # üìå Display DOB & Phone Number\n","    if filename in dob_phone_results:\n","        dob = format_value(dob_phone_results[filename].get('Date of Birth', ''))\n","        phone_numbers = ', '.join(dob_phone_results[filename].get('Phone Numbers', [])) if dob_phone_results[filename].get('Phone Numbers') else ''\n","\n","        print(f\"\\nDate of Birth   : {dob}\")\n","        print(f\"Phone Numbers   : {phone_numbers}\")\n","\n","        # Add to personal_data\n","        personal_data[filename][\"Date of Birth\"] = dob\n","        personal_data[filename][\"Phone Numbers\"] = phone_numbers\n","\n","    # ‚úÖ Display Residential Address\n","    residential_address = residential_address_results.get(filename, \"\")\n","    if isinstance(residential_address, dict):\n","        residential_address = format_value(residential_address.get(\"Address\", \"\"))\n","    else:\n","        residential_address = format_value(residential_address)\n","    print(f\"\\nResidential Address: {residential_address}\")\n","    personal_data[filename][\"Residential Address\"] = residential_address\n","\n","    # ‚úÖ Display Current Address\n","    current_address = current_address_results.get(filename, \"\")\n","    if isinstance(current_address, dict):\n","        current_address = format_value(current_address.get(\"Address\", \"\"))\n","    else:\n","        current_address = format_value(current_address)\n","    print(f\"\\nCurrent Address     : {current_address}\")\n","    personal_data[filename][\"Current Address\"] = current_address\n","\n","    print(\"\\n\" + \"=\" * 60)\n","\n","# Save personal data to JSON file\n","import json\n","import os\n","\n","# Define the target directory\n","upload_dir = \"/content/\"\n","\n","with open('/content/personal data.json', 'w') as f:\n","    json.dump(personal_data, f, indent=4)\n","\n","print(\"\\n‚úÖ Personal data saved to personal data.json\")"]},{"cell_type":"code","source":["import json\n","import os\n","\n","# Path to the JSON file\n","data_file = \"raw data.json\"\n","\n","# Function to display JSON data\n","def display_json(file_path):\n","    if not os.path.exists(file_path):\n","        print(f\"Error: {file_path} not found.\")\n","        return\n","\n","    try:\n","        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","            data = json.load(f)\n","            print(json.dumps(data, indent=4))  # Pretty print JSON data\n","    except json.JSONDecodeError:\n","        print(f\"Error: {file_path} contains invalid JSON.\")\n","    except Exception as e:\n","        print(f\"Error reading {file_path}: {e}\")\n","\n","# Display the JSON data\n","display_json(data_file)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rm1LQ1U9fkwS","executionInfo":{"status":"ok","timestamp":1742464485408,"user_tz":-330,"elapsed":25,"user":{"displayName":"Suryansh Singh","userId":"04524882864171595882"}},"outputId":"a939adb2-0312-4a5e-fab2-be830a0f8b1c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","    \"Naukri_VandanaAgnihotri[2y_0m].pdf\": {\n","        \"education\": [\n","            \"vandanaagnihotri0110@gmail.comEmail\",\n","            \"FaridabadAddress\",\n","            \"J.C. Bose University of Science And\",\n","            \"Maharishi Dayanand UniversityMBA\",\n","            \"BBA\",\n","            \"ExcelOrder to CashAccounts Receivable\"\n","        ],\n","        \"experience\": [\n","            \"EXPERIENCE\",\n","            \"Jun 2022 - Present\",\n","            \"AVC IndiaAccount Executive\",\n","            \"Recording & Processing financial transactions in correct customer account in\",\n","            \"Sap\",\n","            \"Managing cash application profile which comes under O2C Cycle\",\n","            \"Matching payment with the remittance & allocate it with open items or\"\n","        ],\n","        \"skills\": [\n","            \"excel\",\n","            \"ai\",\n","            \"sap\"\n","        ]\n","    },\n","    \"Naukri_RaviPrakashMishra[3y_0m].pdf\": {\n","        \"education\": [\n","            \"Marital  Status  :Single  \\n \\n \\n \\n \\n \\n ACADEMIC  BACKGROUND   \\n  \\n\\uf0b7 Master of Commerce from College\",\n","            \"merce, Patliputra \\nUniversity\",\n","            \"Bachelor  of Commerce  from \\nCalcutta University\"\n","        ],\n","        \"experience\": [\n","            \"professionally managed\",\n","            \"organization where I can apply my strong analytical\"\n","        ],\n","        \"skills\": [\n","            \"excel\",\n","            \"power\",\n","            \"tally\",\n","            \"ai\",\n","            \"ui\",\n","            \"email handling\",\n","            \"zoho books\",\n","            \"quick books\",\n","            \"word\",\n","            \"point\",\n","            \"office\",\n","            \"word, power point, excel\"\n","        ]\n","    },\n","    \"Deepali.pdf\": {\n","        \"education\": [\n","            \"\\u2022 Passed B BA from Maharshi Da yanand  University  in 20 22 with 74.10%\",\n","            \"\\u2022 Passed 12th from HBSE Board  in 20 19 with 82.80%\",\n","            \"\\u2022 Passed 10th from HBSE Board  in 20 17 with 81.20%\"\n","        ],\n","        \"experience\": [\n","            \"Professional who aspires to work in challenging environment where my qualification\",\n","            \"and my\",\n","            \"Experience\",\n","            \"Working in Dygon Tech Solutions as Accounts Executive from Jan 2023 to Till Date\",\n","            \"Key Responsibility Area &\"\n","        ],\n","        \"skills\": [\n","            \"excel\",\n","            \"sap operation\",\n","            \"rest\",\n","            \"office ( word\",\n","            \"word\",\n","            \"office\",\n","            \"sap\"\n","        ]\n","    },\n","    \"Sanjeev Yadav.pdf\": {\n","        \"education\": [\n","            \"IGNOU  UNIVERSITY.\",\n","            \"SGT UNIVERSITY.\",\n","            \"CBSE  BOARD.\"\n","        ],\n","        \"experience\": [\n","            \"Experience 31st Mar 2023 - Present\",\n","            \"Transaction process associate ADDECO INDIA PVT LTD.\",\n","            \"Accounts Payable Accenture\",\n","            \"End to end process knowledge for payment processing under procure to payments domain\",\n","            \"(MEA region).\",\n","            \"Responsibilities :\",\n","            \"1. Creating weekly -monthly and ad-hoc payment proposals including\"\n","        ],\n","        \"skills\": [\n","            \"excel\",\n","            \"erp\",\n","            \"excel)\",\n","            \"ai\",\n","            \"rest\",\n","            \"ml\",\n","            \"word\",\n","            \"sap\",\n","            \"o\\ufb03ce(word\"\n","        ]\n","    },\n","    \"Naukri_ShubhamGupta[1y_5m].pdf\": {\n","        \"education\": [\n","            \"\\u2022 Passed B . Com  from Maharshi Dayanand  University  in 20 22 with 58.30%\",\n","            \"\\u2022 Passed 12th from CBSE  Board  in 20 18 with 6.3 CGPA\",\n","            \"\\u2022 Passed 10th from CBSE  Board  in 20 16 with 7.6 CGPA\"\n","        ],\n","        \"experience\": [\n","            \"Professional who aspires to work in challenging environment where my qualification\",\n","            \"and my\",\n","            \"Experience\",\n","            \"Working in Advance Techcare Solutions as Accounts Executive from Jan 2023 to Till\",\n","            \"Date\",\n","            \"Key Responsibility Area &\"\n","        ],\n","        \"skills\": [\n","            \"excel\",\n","            \"sap operation\",\n","            \"rest\",\n","            \"office ( word\",\n","            \"word\",\n","            \"office\",\n","            \"sap\"\n","        ]\n","    },\n","    \"Naukri_Sachinfogaat[2y_8m].docx\": {\n","        \"education\": [\n","            \"best my abilities and efforts.\\n\\n\\n10th Passed from Board of School\",\n","            \"malayan Garhwal University\"\n","        ],\n","        \"experience\": [\n","            \"professionally by exploring new vistas of life. I assure that given an opportunity. I would work to the best my abilities and efforts.\",\n","            \"10th Passed from Board of School\"\n","        ],\n","        \"skills\": [\n","            \"excel\",\n","            \"erp\",\n","            \"ai\",\n","            \"powerpoint\",\n","            \"word\",\n","            \"office\",\n","            \"sap\"\n","        ]\n","    },\n","    \"Naukri_Rahul[2y_0m].pdf\": {\n","        \"education\": [\n","            \"\\u27a2 Passes 10th in 201 5 from HBSE Board  with 75.60%\",\n","            \"\\u27a2 Passes 10 +2th in 201 7 from HBSE Board  with 83.80%\",\n","            \"\\u27a2 B. Com  in 20 20 from  Maharshi Dayanand  University  with 65.52%\"\n","        ],\n","        \"experience\": [\n","            \"EXPERIENCE:\",\n","            \"Workin g as an Accountan t Executive ( P2P \\u2013 Procure to Pay ) in White Cloud Sourcing from Jan\",\n","            \"2022 to Till D ate\",\n","            \"JOB RESPONSIBILITIES ( P2P)\",\n","            \"\\u27a2 Verify & validate the invoices before processing in system as per P2P requirements\",\n","            \"\\u27a2 Responsible for v endor inv oice process ing in system a s per TDS & GST compliance s\",\n","            \"\\u27a2 Process PO & Non -PO based invoices in SAP on daily basis\"\n","        ],\n","        \"skills\": [\n","            \"excel\",\n","            \"ai\",\n","            \"ui\",\n","            \"word\",\n","            \"office\",\n","            \"sap\"\n","        ]\n","    },\n","    \"Naukri_SarthakBhatnagar[3y_6m].pdf\": {\n","        \"education\": [\n","            \"Qualification  Institute / Board / University  Year\",\n","            \"M.com  CCSU University  2020 -22\",\n","            \"B.com  CCSU University  2019 -20\",\n","            \"Senior Secondary  St Mary\\u2019s Academy (CBSE)  2015 -16\",\n","            \"10th Standard  St Mary\\u2019s Academy (CBSE)  2012 -13\",\n","            \"\\uf0b7 Accounts Payable .\",\n","            \"Hobbies:  Listening  News,  songs  and traveling\",\n","            \"[Sarthak Bhatnagar]\"\n","        ],\n","        \"experience\": [\n","            \"Internal Auditing. Nationality : Indian Hobbies: Listening News, songs and traveling Language Known : English, Hindi Strength: Fast & self-directed learner, work effectively, Independently , Confident. Date of Birth: 21st Sep 1997. Place: Noida Signature Date: [Sarthak Bhatnagar]\"\n","        ],\n","        \"skills\": [\n","            \"excel\",\n","            \"erp\",\n","            \"tally\",\n","            \"ai\",\n","            \"ui\"\n","        ]\n","    }\n","}\n"]}]},{"cell_type":"markdown","metadata":{"id":"5-wGHtnwxlSL"},"source":["# **Importing in Excel Sheet**"]},{"cell_type":"code","source":["import pandas as pd\n","import json\n","import os\n","\n","# Define the target directory\n","upload_dir = \"/content/\"\n","excel_file = os.path.join(upload_dir, \"Resume Data.xlsx\")\n","\n","# Load personal data\n","with open(os.path.join(upload_dir, 'personal data.json'), 'r') as f:\n","    personal_data = json.load(f)\n","\n","# Load arranged data\n","with open(os.path.join(upload_dir, 'arranged data.json'), 'r') as f:\n","    arranged_data = json.load(f)\n","\n","# Load raw data for skills\n","with open(os.path.join(upload_dir, 'raw data.json'), 'r') as f:\n","    raw_data = json.load(f)\n","\n","# Create DataFrame for Excel with unique column names\n","columns = [\n","    \"First Name\", \"Middle Name\", \"Last Name\", \"Father Name\", \"DOB\", \"Gender\", \"Email ID\", \"Languages\",\n","    \"Residential Address\", \"Current Address\", \"Phone No\", \"Total Experience\",\n","    \"Job Title 1\", \"Company 1\", \"Office Location 1\", \"Job Description 1\", \"Exp From Date 1\", \"Exp To Date 1\",\n","    \"Job Title 2\", \"Company 2\", \"Office Location 2\", \"Job Description 2\", \"Exp From Date 2\", \"Exp To Date 2\",\n","    \"Job Title 3\", \"Company 3\", \"Office Location 3\", \"Job Description 3\", \"Exp From Date 3\", \"Exp To Date 3\",\n","    \"Job Title 4\", \"Company 4\", \"Office Location 3\", \"Job Description 4\", \"Exp From Date 4\", \"Exp To Date 4\",\n","    \"Institution 1\", \"Major 1\", \"Degree 1\", \"School Location 1\", \"Description 1\", \"Edu From Date 1\", \"Edu To Date 1\",\n","    \"Institution 2\", \"Major 2\", \"Degree 2\", \"School Location 2\", \"Description 2\", \"Edu From Date 2\", \"Edu To Date 2\",\n","    \"Institution 3\", \"Major 3\", \"Degree 3\", \"School Location 3\", \"Description 3\", \"Edu From Date 3\", \"Edu To Date 3\",\n","    \"Institution 4\", \"Major 4\", \"Degree 4\", \"School Location 4\", \"Description 4\", \"Edu From Date 4\", \"Edu To Date 4\",\n","    \"Institution 5\", \"Major 5\", \"Degree 5\", \"School Location 5\", \"Description 5\", \"Edu From Date 5\", \"Edu To Date 5\",\n","    \"Institution 6\", \"Major 6\", \"Degree 6\", \"School Location 6\", \"Description 6\", \"Edu From Date 6\", \"Edu To Date 6\",\n","    \"Certification 1\", \"Certification 2\", \"Certification 3\",\n","    \"Key Skill 1\", \"Key Skill 2\", \"Key Skill 3\", \"Key Skill 4\", \"Key Skill 5\", \"Key Skill 6\", \"Key Skill 7\",\n","    \"Key Skill 8\", \"Key Skill 9\", \"Key Skill 10\", \"Key Skill 11\", \"Key Skill 12\", \"Key Skill 13\", \"Key Skill 14\",\n","    \"Key Skill 15\", \"Key Skill 16\"\n","]\n","\n","# Verify no duplicate column names\n","if len(columns) != len(set(columns)):\n","    duplicate_columns = [col for col in columns if columns.count(col) > 1]\n","    print(f\"Warning: Duplicate column names found: {duplicate_columns}\")\n","else:\n","    print(\"No duplicate column names detected.\")\n","\n","# Create an empty list to collect all row data\n","all_rows = []\n","\n","# Process data\n","for filename in personal_data.keys():\n","    row_data = {col: \"\" for col in columns}\n","\n","    # Personal Data\n","    row_data.update(personal_data[filename])\n","\n","    # Explicitly map Father Name, DOB, and Phone No to handle key mismatches\n","    personal_entry = personal_data[filename]\n","    row_data[\"Father Name\"] = personal_entry.get(\"Father Name\", personal_entry.get(\"FatherName\", personal_entry.get(\"Father's Name\", \"\")))\n","    row_data[\"DOB\"] = personal_entry.get(\"DOB\", personal_entry.get(\"Date of Birth\", personal_entry.get(\"DateofBirth\", \"\")))\n","    row_data[\"Phone No\"] = personal_entry.get(\"Phone No\", personal_entry.get(\"Phone\", personal_entry.get(\"PhoneNumber\", personal_entry.get(\"Phone Numbers\", \"\"))))\n","\n","    # Arranged Data (Education & Experience) as primary source\n","    arranged_entry = arranged_data.get(filename, {})\n","    raw_entry = raw_data.get(filename, {})\n","\n","    # Handle education data\n","    edu_data = arranged_entry.get(\"Education\", {})\n","    raw_edu_data = raw_entry.get(\"education\", [])\n","\n","    if edu_data and len(edu_data) > 0:  # If arranged data exists and is not empty, use it\n","        for i, key in enumerate(list(edu_data.keys())[:6], 1):\n","            edu = edu_data[key]\n","            row_data[f\"Institution {i}\"] = edu.get(\"University/Board\", \"\")\n","            row_data[f\"Degree {i}\"] = edu.get(\"Degree\", \"\")\n","            row_data[f\"Edu From Date {i}\"] = edu.get(\"Year\", \"\")\n","            row_data[f\"Edu To Date {i}\"] = edu.get(\"Year\", \"\")\n","            row_data[f\"Description {i}\"] = edu.get(\"Percentage\", \"\")\n","    elif raw_edu_data:  # Fallback to raw data if arranged data is empty or missing\n","        for i, edu in enumerate(raw_edu_data[:6], 1):\n","            row_data[f\"Description {i}\"] = edu if isinstance(edu, str) else \"\"\n","\n","    # Handle experience data\n","    exp_data = arranged_entry.get(\"Experience\", {})\n","    raw_exp_data = raw_entry.get(\"experience\", [])\n","\n","    if exp_data and len(exp_data) > 0:  # If arranged data exists and is not empty, use it\n","        for i, key in enumerate(list(exp_data.keys())[:4], 1):\n","            exp = exp_data[key]\n","            row_data[f\"Company {i}\"] = exp.get(\"Company\", \"\")\n","            row_data[f\"Job Title {i}\"] = exp.get(\"Role\", \"\")\n","            row_data[f\"Exp From Date {i}\"] = exp.get(\"Start Date\", \"\")\n","            row_data[f\"Exp To Date {i}\"] = exp.get(\"End Date\", \"\")\n","            row_data[f\"Job Description {i}\"] = exp.get(\"Raw experience\", \"\")\n","    elif raw_exp_data:  # Fallback to raw data if arranged data is empty or missing\n","        for i, exp in enumerate(raw_exp_data[:4], 1):\n","            row_data[f\"Job Description {i}\"] = exp if isinstance(exp, str) else \"\"\n","\n","    # Skill Data from Raw Data\n","    skills = raw_data.get(filename, {}).get(\"skills\", [])\n","    for i, skill in enumerate(skills[:16], 1):\n","        # Extract 'content' if skill is a dictionary, otherwise use the skill as-is\n","        row_data[f\"Key Skill {i}\"] = skill.get(\"content\", skill) if isinstance(skill, dict) else skill\n","\n","    # Add row to our collection\n","    all_rows.append(row_data)\n","\n","# Create DataFrame from all rows at once\n","df = pd.DataFrame(all_rows, columns=columns)\n","\n","# Save the DataFrame to Excel using openpyxl\n","with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n","    df.to_excel(writer, index=False, sheet_name='Resume Data')\n","\n","print(f\"\\n‚úÖ Data successfully saved to {excel_file}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JHashs11OnQn","executionInfo":{"status":"ok","timestamp":1742798553776,"user_tz":-330,"elapsed":480,"user":{"displayName":"Suryansh Singh","userId":"04524882864171595882"}},"outputId":"590f5bdc-2b8f-4693-b0c6-be26f5922efb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: Duplicate column names found: ['Office Location 3', 'Office Location 3']\n","\n","‚úÖ Data successfully saved to /content/Resume Data.xlsx\n"]}]},{"cell_type":"markdown","metadata":{"id":"_dzTg-2n-w3t"},"source":["**Download**"]},{"cell_type":"code","source":["from google.colab import files\n","import shutil\n","import os\n","\n","# Define the file path\n","excel_file_path = \"/content/Resume Data.xlsx\"\n","\n","# Check if the file exists\n","if os.path.exists(excel_file_path):\n","    # Download the file\n","    files.download(excel_file_path)\n","    print(f\"‚úÖ File '{excel_file_path}' downloaded successfully.\")\n","else:\n","    print(f\"‚ùå File '{excel_file_path}' not found. Please ensure the file is created before downloading.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"Kjj1jTLYpAsa","executionInfo":{"status":"ok","timestamp":1742798558471,"user_tz":-330,"elapsed":31,"user":{"displayName":"Suryansh Singh","userId":"04524882864171595882"}},"outputId":"30125040-cc37-4470-ad66-4ad41e3bc6ee"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_0a502f06-9549-42ed-974d-90ac6b2471c1\", \"Resume Data.xlsx\", 8495)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ File '/content/Resume Data.xlsx' downloaded successfully.\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}